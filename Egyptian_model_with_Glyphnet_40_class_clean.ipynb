{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Egyptian_model_with_Glyphnet_40_class_clean.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iEiGSblxJzW"
      },
      "source": [
        "import os, os.path\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from pathlib import Path\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd6KU8VMz6J3"
      },
      "source": [
        "Module: Load_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lWX0BW_w_kC"
      },
      "source": [
        "def load_data(hieroglyph_directory_path, batch_size=20, num_workers=0):\n",
        "    train_dir = os.path.join(hieroglyph_directory_path, 'train/')\n",
        "    test_dir = os.path.join(hieroglyph_directory_path, 'test/')\n",
        "\n",
        "    classes = []\n",
        "\n",
        "    for filename in os.listdir(train_dir):\n",
        "        if filename == '.DS_Store':\n",
        "            pass\n",
        "        else:\n",
        "            classes.append(filename)\n",
        "\n",
        "    classes.sort()\n",
        "\n",
        "    # print(\"Our classes:\", classes)\n",
        "    # print(len(classes))\n",
        "\n",
        "    data_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                                transforms.RandomApply([transforms.RandomHorizontalFlip()]),\n",
        "                                                transforms.RandomRotation(degrees=(-10, 10)),\n",
        "                                                transforms.RandomAffine(degrees=0, translate=(.1, .1)),\n",
        "                                                transforms.RandomApply([transforms.ColorJitter(brightness=(1, 1.2),\n",
        "                                                                                                contrast=(1, 1.5),\n",
        "                                                                                                saturation=(1, 1.5),\n",
        "                                                                                                hue=(0, 0.5))]),\n",
        "                                                transforms.RandomErasing(p=0.5, scale=(0.05, 0.05), ratio=(0.3, 3.3), value=0,\n",
        "                                                                          inplace=False),\n",
        "                                                transforms.Resize((100, 100)),\n",
        "                                                transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "    train_data = datasets.ImageFolder(train_dir, transform=data_transform)\n",
        "    test_data = datasets.ImageFolder(test_dir, transform=data_transform)\n",
        "\n",
        "    # print('Num training images: ', len(train_data))\n",
        "    # print('Num test images: ', len(test_data))\n",
        "\n",
        "    # prepare data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "                                               num_workers=num_workers, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
        "                                              num_workers=num_workers, shuffle=True)\n",
        "\n",
        "    return train_loader, test_loader, classes"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4sXUrPPz7iF"
      },
      "source": [
        "Module: Train_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2N_rnKLxN5c"
      },
      "source": [
        "def train_model(train_loader, optimizer, conv_net, criterion, my_lr_scheduler, n_epochs):\n",
        "    # track training loss over time\n",
        "    losses = []\n",
        "    \n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "        # keep track of training and validation loss\n",
        "        train_loss = 0.0\n",
        "\n",
        "        # model by default is set to train\n",
        "        for batch_i, (data, target) in enumerate(train_loader):\n",
        "            # move tensors to GPU if CUDA is available\n",
        "            if train_on_gpu:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = conv_net(data)\n",
        "            # calculate the batch loss\n",
        "            loss = criterion(output, target)\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "            # update training loss\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            my_lr_scheduler.step()\n",
        "\n",
        "            if batch_i % 20 == 19:  # print training loss every specified number of mini-batches\n",
        "                print('Epoch %d, Batch %d loss: %.16f' %\n",
        "                    (epoch, batch_i + 1, train_loss / 20))\n",
        "                losses.append(train_loss / 20)\n",
        "                train_loss = 0.0\n",
        "        \n",
        "        \n",
        "\n",
        "    return conv_net, losses"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Rrxo5RDz96U"
      },
      "source": [
        "Module: Test_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay-HlMJnxU9v"
      },
      "source": [
        "def test_model(classes, conv_net, test_loader, criterion):\n",
        "    # track test loss\n",
        "    test_loss = 0.0\n",
        "    class_correct = list(0. for i in range(len(classes)))\n",
        "    class_total = list(0. for i in range(len(classes)))\n",
        "\n",
        "    conv_net.eval()  # eval mode\n",
        "\n",
        "    labels = []\n",
        "    predictions = []\n",
        "    # iterate over test data\n",
        "    for data, target in test_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = conv_net(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # update  test loss\n",
        "        test_loss += loss.item() * data.size(0)\n",
        "        # convert output probabilities to predicted class\n",
        "        _, pred = torch.max(output, 1)\n",
        "        # compare predictions to true label\n",
        "        correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "        # calculate test accuracy for each object class\n",
        "        for i in range(len(target.data)):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "        \n",
        "        # Will be used for calculating Recall, Precision, and F1-score\n",
        "        labels.extend(target.data.view_as(pred).tolist())\n",
        "        predictions.extend(pred.tolist())\n",
        "\n",
        "\n",
        "    # calculate avg test loss\n",
        "    test_loss = test_loss / len(test_loader.dataset)\n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "    return test_loss, class_correct, class_total, labels, predictions"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHh3PXE80Asa"
      },
      "source": [
        "Check whether CUDA is available (Change runtime type if not)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9TYPQH7x4zw",
        "outputId": "acb621c2-6328-48d0-c6ea-7900259a25e1"
      },
      "source": [
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lYw8EKVx8Q7"
      },
      "source": [
        "Load Hieroglyph Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laLvoRy1yewl",
        "outputId": "6268df88-0bf5-4214-8de1-35002e4c1a9b"
      },
      "source": [
        "# Connecting and Mounting to the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odaoZt9GyfUt"
      },
      "source": [
        "data_dir = '/content/drive/MyDrive/EgyptianHieroglyphDataset_Original_Clean/'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF22MAnmyiC1",
        "outputId": "c84761df-9b78-4bde-b2f5-d51be4a0321b"
      },
      "source": [
        "hieroglyph_for_train = []\n",
        "file_count_list = []\n",
        "\n",
        "for name in os.listdir('/content/drive/MyDrive/EgyptianHieroglyphDataset_Original_Clean/train/'):\n",
        "  path, dirs, files = next(os.walk(\"/content/drive/MyDrive/EgyptianHieroglyphDataset_Original_Clean/train/\"+name))\n",
        "  file_count = len(files)\n",
        "  print(name, file_count)\n",
        "  file_count_list.append(file_count)\n",
        "  hieroglyph_for_train.append(name)\n",
        "\n",
        "hieroglyph_dict = dict(zip(hieroglyph_for_train, file_count_list))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y5 6\n",
            "I9 116\n",
            "O34 15\n",
            "V13 63\n",
            "U7 3\n",
            "D46 40\n",
            "E34 97\n",
            "G1 28\n",
            "V31 106\n",
            "S34 8\n",
            "D36 47\n",
            "Q1 13\n",
            "V30 6\n",
            "F35 2\n",
            "O4 11\n",
            "M23 30\n",
            "S29 212\n",
            "R8 53\n",
            "W11 4\n",
            "X1 185\n",
            "Y1 1\n",
            "D21 146\n",
            "D2 19\n",
            "E23 8\n",
            "X8 4\n",
            "Z1 39\n",
            "D4 29\n",
            "V28 28\n",
            "I10 32\n",
            "O1 16\n",
            "M17 291\n",
            "F31 6\n",
            "G43 157\n",
            "D58 28\n",
            "O49 10\n",
            "G17 156\n",
            "R4 2\n",
            "Q3 61\n",
            "W24 31\n",
            "N35 358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeQQ7PjZzKd9"
      },
      "source": [
        "Number of images for each hieroglyph "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "W6IzWR2RyjxM",
        "outputId": "080c2aa1-aed7-4859-dfff-8585a5ff6fea"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\"Hieroglyph\":hieroglyph_for_train, \"Count\":file_count_list})\n",
        "\n",
        "df_sorted= df.sort_values('Count',ascending=False)\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "# make bar plot with matplotlib\n",
        "plt.bar('Hieroglyph', 'Count',data=df_sorted)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 40 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3oOm8rSx7tp"
      },
      "source": [
        "# Number of images processed in a single training\n",
        "batch_size = 20\n",
        "num_workers = 0\n",
        "\n",
        "# The load_data function is from hieroglyph_data_preparation python file\n",
        "train_loader, test_loader, classes = load_data(data_dir)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJm8sGiBx_q5"
      },
      "source": [
        "ResNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii1UaV8yPvW_"
      },
      "source": [
        "class SeparableConv2d(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,kernel_size=3,stride=1,padding=0,dilation=1,bias=False):\n",
        "        super(SeparableConv2d,self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n",
        "        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hnE4jAAPbnX"
      },
      "source": [
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "oXpu43oPMyor",
        "outputId": "499ff35a-d669-4dd6-c75f-27ae86811c5a"
      },
      "source": [
        "'''\n",
        "class Glyphnet(torch.nn.Module):   \n",
        "    def __init__(self):\n",
        "        super(Glyphnet, self).__init__()\n",
        "\n",
        "        self.conv1 = Conv2d(3, 64, kernel_size=3, stride=1, padding=0)\n",
        "        self.maxpool1 = MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = Conv2d(64, 64, kernel_size=3, stride=1, padding=0)\n",
        "        self.maxpool2 = MaxPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "        self.fc1 = Linear(23 * 23 * 64, 40)\n",
        "\n",
        "    # Defining the forward pass    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "'''"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nclass Glyphnet(torch.nn.Module):   \\n    def __init__(self):\\n        super(Glyphnet, self).__init__()\\n\\n        self.conv1 = Conv2d(3, 64, kernel_size=3, stride=1, padding=0)\\n        self.maxpool1 = MaxPool2d(kernel_size=2, stride=2)\\n        self.conv2 = Conv2d(64, 64, kernel_size=3, stride=1, padding=0)\\n        self.maxpool2 = MaxPool2d(kernel_size=3, stride=2)\\n\\n        self.fc1 = Linear(23 * 23 * 64, 40)\\n\\n    # Defining the forward pass    \\n    def forward(self, x):\\n        x = self.conv1(x)\\n        x = self.maxpool1(x)\\n        x = self.conv2(x)\\n        x = self.maxpool2(x)\\n\\n        x = x.view(x.size(0), -1)\\n        x = self.fc1(x)\\n        return x\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mik3vFrCSeTd"
      },
      "source": [
        "class Glyphnet(torch.nn.Module):\n",
        "   def __init__(self):\n",
        "      super(Glyphnet, self).__init__()\n",
        "      \n",
        "      self.conv1 = torch.nn.Conv2d(3, 64, kernel_size = 3, stride = 1, padding = 0)\n",
        "      self.conv2 = torch.nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 0)\n",
        "      self.maxpool = torch.nn.MaxPool2d(kernel_size = 3, stride = 1, padding = 0)\n",
        "      self.avgpool = torch.nn.AvgPool2d(kernel_size=66)\n",
        "\n",
        "      self.bn1 = torch.nn.BatchNorm2d(64)\n",
        "      self.bn2 = torch.nn.BatchNorm2d(128)\n",
        "      self.bn3 = torch.nn.BatchNorm2d(256)\n",
        "      self.bn4 = torch.nn.BatchNorm2d(512)\n",
        "\n",
        "      self.separableconv1 = SeparableConv2d(64,128)\n",
        "      self.separableconv2 = SeparableConv2d(128,128)\n",
        "      self.separableconv3 = SeparableConv2d(128,256)\n",
        "      self.separableconv4 = SeparableConv2d(256,256)\n",
        "      self.separableconv5 = SeparableConv2d(256,512)\n",
        "\n",
        "      self.drop_layer = nn.Dropout(p=0.15)\n",
        "      self.fc1 = torch.nn.Linear(512, 40) # Should change 64 part\n",
        "      self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "   def forward(self, x): # Input: 100 x 100 x 3\n",
        "      \n",
        "      # First block\n",
        "      x = self.bn1(self.conv1(x)) # Output: 98 x 98 x 64\n",
        "      x = F.relu(self.maxpool(x)) # Output: 96 x 96 x 64\n",
        "      x = self.bn1(self.conv2(x)) # Output: 94 x 94 x 64\n",
        "      x = F.relu(self.maxpool(x)) # Output: 92 x 92 x 64\n",
        "      \n",
        "      # Second block\n",
        "      x = F.relu(self.bn2(self.separableconv1(x))) # Output: 90 x 90 x 128\n",
        "      x = self.bn2(self.separableconv2(x)) # Output: 88 x 88 x 128\n",
        "      x = F.relu(self.maxpool(x)) # Output: 86 x 86 x 128\n",
        "\n",
        "      # Third block\n",
        "      x = F.relu(self.bn2(self.separableconv2(x))) # Output: 84 x 84 x 128\n",
        "      x = self.bn2(self.separableconv2(x)) # Output: 82 x 82 x 128\n",
        "      x = F.relu(self.maxpool(x)) # Output: 80 x 80 x 128\n",
        "\n",
        "      # Fourth block\n",
        "      x = F.relu(self.bn3(self.separableconv3(x))) # Output: 78 x 78 x 256\n",
        "      x = self.bn3(self.separableconv4(x)) # Output: 76 x 76 x 256\n",
        "      x = F.relu(self.maxpool(x)) # Output: 74 x 74 x 256\n",
        "\n",
        "      # Fifth block\n",
        "      x = F.relu(self.bn3(self.separableconv4(x))) # Output: 72 x 72 x 256\n",
        "      x = self.bn3(self.separableconv4(x)) # Output: 70 x 70 x 256\n",
        "      x = F.relu(self.maxpool(x)) # Output: 68 x 68 x 256\n",
        "\n",
        "      # Sixth block\n",
        "      x = F.relu(self.bn4(self.separableconv5(x))) # Output: 66 x 66 x 512\n",
        "\n",
        "      x = self.avgpool(x) # Global Average Pooling; Output: 512\n",
        "\n",
        "      x = self.drop_layer(x) # Dropout Layer; Output: 512\n",
        "      \n",
        "      # Fully Connected Layer; Output: 512\n",
        "      x = x.view(x.size(0), -1)\n",
        "      x = self.fc1(x)\n",
        "\n",
        "      x = self.softmax(x) # Softmax; Output: 512\n",
        "      \n",
        "      return(x)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkLXVJGkxemC"
      },
      "source": [
        "# Whether to extract features with the model\n",
        "feature_extract = False\n",
        "# Other selections\n",
        "loss_function = \"cross-entropy\"\n",
        "model_selection = \"glyphnet\"\n",
        "optim_selection = \"Adam\"\n",
        "\n",
        "# Load the model\n",
        "if model_selection == \"glyphnet\":\n",
        "    glyphnet = Glyphnet()\n",
        "\n",
        "# if GPU is available, move the model to GPU\n",
        "if train_on_gpu:\n",
        "    glyphnet.cuda()\n",
        "\n",
        "# Specify loss function (categorical cross-entropy)\n",
        "if loss_function == \"cross-entropy\":\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Specify optimizer (Adam) and learning rate = 0.001\n",
        "if optim_selection == \"Adam\":\n",
        "    optimizer = optim.Adam(glyphnet.parameters(), lr=0.001)\n",
        "\n",
        "# Exponential Decay to strengthen learning\n",
        "decayRate = 0.999\n",
        "my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2vnErB1yHmM"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOB03noWyEvQ",
        "outputId": "f9e69cfe-749a-49dc-907b-38d3320ad3ed"
      },
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 100\n",
        "\n",
        "# The train_model function is from model_training python file\n",
        "glyphnet, train_losses = train_model(train_loader, optimizer, glyphnet, criterion, my_lr_scheduler, n_epochs)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 20 loss: 3.5870957612991332\n",
            "Epoch 1, Batch 40 loss: 3.4799970507621767\n",
            "Epoch 1, Batch 60 loss: 3.4570271134376527\n",
            "Epoch 1, Batch 80 loss: 3.3436991691589357\n",
            "Epoch 1, Batch 100 loss: 3.3432934641838075\n",
            "Epoch 1, Batch 120 loss: 3.3222361207008362\n",
            "Epoch 2, Batch 20 loss: 3.2646391391754150\n",
            "Epoch 2, Batch 40 loss: 3.3010222911834717\n",
            "Epoch 2, Batch 60 loss: 3.3121579289436340\n",
            "Epoch 2, Batch 80 loss: 3.3248308897018433\n",
            "Epoch 2, Batch 100 loss: 3.2636739373207093\n",
            "Epoch 2, Batch 120 loss: 3.2704591631889341\n",
            "Epoch 3, Batch 20 loss: 3.3021568059921265\n",
            "Epoch 3, Batch 40 loss: 3.3211640954017638\n",
            "Epoch 3, Batch 60 loss: 3.2613549232482910\n",
            "Epoch 3, Batch 80 loss: 3.2529952287673951\n",
            "Epoch 3, Batch 100 loss: 3.2737946510314941\n",
            "Epoch 3, Batch 120 loss: 3.2678864598274231\n",
            "Epoch 4, Batch 20 loss: 3.2957405328750609\n",
            "Epoch 4, Batch 40 loss: 3.2772415637969972\n",
            "Epoch 4, Batch 60 loss: 3.2654931187629699\n",
            "Epoch 4, Batch 80 loss: 3.2761029243469237\n",
            "Epoch 4, Batch 100 loss: 3.2829301238059996\n",
            "Epoch 4, Batch 120 loss: 3.2363736391067506\n",
            "Epoch 5, Batch 20 loss: 3.2593623876571653\n",
            "Epoch 5, Batch 40 loss: 3.2360988020896913\n",
            "Epoch 5, Batch 60 loss: 3.2890442252159118\n",
            "Epoch 5, Batch 80 loss: 3.2678818941116332\n",
            "Epoch 5, Batch 100 loss: 3.2555414676666259\n",
            "Epoch 5, Batch 120 loss: 3.2729258775711059\n",
            "Epoch 6, Batch 20 loss: 3.2318464994430540\n",
            "Epoch 6, Batch 40 loss: 3.2744404315948485\n",
            "Epoch 6, Batch 60 loss: 3.2613769292831423\n",
            "Epoch 6, Batch 80 loss: 3.2756840229034423\n",
            "Epoch 6, Batch 100 loss: 3.2870423674583433\n",
            "Epoch 6, Batch 120 loss: 3.2576226115226747\n",
            "Epoch 7, Batch 20 loss: 3.2940739631652831\n",
            "Epoch 7, Batch 40 loss: 3.2106870174407960\n",
            "Epoch 7, Batch 60 loss: 3.2535958409309389\n",
            "Epoch 7, Batch 80 loss: 3.2581718802452087\n",
            "Epoch 7, Batch 100 loss: 3.2648390054702761\n",
            "Epoch 7, Batch 120 loss: 3.2734247326850889\n",
            "Epoch 8, Batch 20 loss: 3.2939527750015261\n",
            "Epoch 8, Batch 40 loss: 3.2492844700813293\n",
            "Epoch 8, Batch 60 loss: 3.2679821968078615\n",
            "Epoch 8, Batch 80 loss: 3.2145842313766479\n",
            "Epoch 8, Batch 100 loss: 3.2305216312408449\n",
            "Epoch 8, Batch 120 loss: 3.2788161873817443\n",
            "Epoch 9, Batch 20 loss: 3.2711379408836363\n",
            "Epoch 9, Batch 40 loss: 3.2782634973525999\n",
            "Epoch 9, Batch 60 loss: 3.2612036228179933\n",
            "Epoch 9, Batch 80 loss: 3.2047119855880739\n",
            "Epoch 9, Batch 100 loss: 3.2618306517601012\n",
            "Epoch 9, Batch 120 loss: 3.2617038011550905\n",
            "Epoch 10, Batch 20 loss: 3.2394843578338621\n",
            "Epoch 10, Batch 40 loss: 3.2626409292221070\n",
            "Epoch 10, Batch 60 loss: 3.2753444314002991\n",
            "Epoch 10, Batch 80 loss: 3.2263593673706055\n",
            "Epoch 10, Batch 100 loss: 3.2562023758888246\n",
            "Epoch 10, Batch 120 loss: 3.2623940467834474\n",
            "Epoch 11, Batch 20 loss: 3.2485686540603638\n",
            "Epoch 11, Batch 40 loss: 3.2533106565475465\n",
            "Epoch 11, Batch 60 loss: 3.2422646999359133\n",
            "Epoch 11, Batch 80 loss: 3.2382889270782469\n",
            "Epoch 11, Batch 100 loss: 3.2580144405364990\n",
            "Epoch 11, Batch 120 loss: 3.2850493669509886\n",
            "Epoch 12, Batch 20 loss: 3.2766750097274779\n",
            "Epoch 12, Batch 40 loss: 3.2410226702690124\n",
            "Epoch 12, Batch 60 loss: 3.2811664462089540\n",
            "Epoch 12, Batch 80 loss: 3.2347561120986938\n",
            "Epoch 12, Batch 100 loss: 3.2214890480041505\n",
            "Epoch 12, Batch 120 loss: 3.2542896747589110\n",
            "Epoch 13, Batch 20 loss: 3.2413491487503050\n",
            "Epoch 13, Batch 40 loss: 3.2704953789710998\n",
            "Epoch 13, Batch 60 loss: 3.2401793122291567\n",
            "Epoch 13, Batch 80 loss: 3.2252275347709656\n",
            "Epoch 13, Batch 100 loss: 3.2540506362915038\n",
            "Epoch 13, Batch 120 loss: 3.2525120854377745\n",
            "Epoch 14, Batch 20 loss: 3.2519334912300111\n",
            "Epoch 14, Batch 40 loss: 3.2487853407859801\n",
            "Epoch 14, Batch 60 loss: 3.2423535346984864\n",
            "Epoch 14, Batch 80 loss: 3.2314443349838258\n",
            "Epoch 14, Batch 100 loss: 3.2247603654861452\n",
            "Epoch 14, Batch 120 loss: 3.2069169163703917\n",
            "Epoch 15, Batch 20 loss: 3.2003304004669189\n",
            "Epoch 15, Batch 40 loss: 3.2119479775428772\n",
            "Epoch 15, Batch 60 loss: 3.2221310019493101\n",
            "Epoch 15, Batch 80 loss: 3.2479058384895323\n",
            "Epoch 15, Batch 100 loss: 3.2074652671813966\n",
            "Epoch 15, Batch 120 loss: 3.2318604826927184\n",
            "Epoch 16, Batch 20 loss: 3.2026222586631774\n",
            "Epoch 16, Batch 40 loss: 3.2580692648887633\n",
            "Epoch 16, Batch 60 loss: 3.1656021952629088\n",
            "Epoch 16, Batch 80 loss: 3.1823940873146057\n",
            "Epoch 16, Batch 100 loss: 3.1783617854118349\n",
            "Epoch 16, Batch 120 loss: 3.1983480572700502\n",
            "Epoch 17, Batch 20 loss: 3.1445077419281007\n",
            "Epoch 17, Batch 40 loss: 3.1670761108398438\n",
            "Epoch 17, Batch 60 loss: 3.1597965955734253\n",
            "Epoch 17, Batch 80 loss: 3.2258188128471375\n",
            "Epoch 17, Batch 100 loss: 3.2024561285972597\n",
            "Epoch 17, Batch 120 loss: 3.1849984407424925\n",
            "Epoch 18, Batch 20 loss: 3.1572113394737245\n",
            "Epoch 18, Batch 40 loss: 3.1494158148765563\n",
            "Epoch 18, Batch 60 loss: 3.1515588402748107\n",
            "Epoch 18, Batch 80 loss: 3.1701139330863954\n",
            "Epoch 18, Batch 100 loss: 3.2039265751838686\n",
            "Epoch 18, Batch 120 loss: 3.1881451249122619\n",
            "Epoch 19, Batch 20 loss: 3.1842746138572693\n",
            "Epoch 19, Batch 40 loss: 3.1795921444892885\n",
            "Epoch 19, Batch 60 loss: 3.1954289674758911\n",
            "Epoch 19, Batch 80 loss: 3.1432566761970522\n",
            "Epoch 19, Batch 100 loss: 3.1414561033248902\n",
            "Epoch 19, Batch 120 loss: 3.1566789388656615\n",
            "Epoch 20, Batch 20 loss: 3.1835323572158813\n",
            "Epoch 20, Batch 40 loss: 3.1691585779190063\n",
            "Epoch 20, Batch 60 loss: 3.1568782091140748\n",
            "Epoch 20, Batch 80 loss: 3.1864473581314088\n",
            "Epoch 20, Batch 100 loss: 3.1633733987808226\n",
            "Epoch 20, Batch 120 loss: 3.1252830505371092\n",
            "Epoch 21, Batch 20 loss: 3.1701992392539977\n",
            "Epoch 21, Batch 40 loss: 3.1406423091888427\n",
            "Epoch 21, Batch 60 loss: 3.1200734376907349\n",
            "Epoch 21, Batch 80 loss: 3.1517920613288881\n",
            "Epoch 21, Batch 100 loss: 3.1672692656517030\n",
            "Epoch 21, Batch 120 loss: 3.1699934482574461\n",
            "Epoch 22, Batch 20 loss: 3.1276938915252686\n",
            "Epoch 22, Batch 40 loss: 3.1304121375083924\n",
            "Epoch 22, Batch 60 loss: 3.1602273702621462\n",
            "Epoch 22, Batch 80 loss: 3.1575505733489990\n",
            "Epoch 22, Batch 100 loss: 3.1819196939468384\n",
            "Epoch 22, Batch 120 loss: 3.1316072225570677\n",
            "Epoch 23, Batch 20 loss: 3.1241466641426086\n",
            "Epoch 23, Batch 40 loss: 3.1372198104858398\n",
            "Epoch 23, Batch 60 loss: 3.1492444515228271\n",
            "Epoch 23, Batch 80 loss: 3.1535645127296448\n",
            "Epoch 23, Batch 100 loss: 3.1617851734161375\n",
            "Epoch 23, Batch 120 loss: 3.1466017842292784\n",
            "Epoch 24, Batch 20 loss: 3.1838995814323425\n",
            "Epoch 24, Batch 40 loss: 3.1112894892692564\n",
            "Epoch 24, Batch 60 loss: 3.1279822707176210\n",
            "Epoch 24, Batch 80 loss: 3.1370017886161805\n",
            "Epoch 24, Batch 100 loss: 3.1199505329132080\n",
            "Epoch 24, Batch 120 loss: 3.1788844943046568\n",
            "Epoch 25, Batch 20 loss: 3.1297414898872375\n",
            "Epoch 25, Batch 40 loss: 3.1471950531005861\n",
            "Epoch 25, Batch 60 loss: 3.1253449559211730\n",
            "Epoch 25, Batch 80 loss: 3.1648056030273439\n",
            "Epoch 25, Batch 100 loss: 3.1743267536163331\n",
            "Epoch 25, Batch 120 loss: 3.1200666904449461\n",
            "Epoch 26, Batch 20 loss: 3.1580346703529356\n",
            "Epoch 26, Batch 40 loss: 3.1312452197074889\n",
            "Epoch 26, Batch 60 loss: 3.1514162063598632\n",
            "Epoch 26, Batch 80 loss: 3.1562958478927614\n",
            "Epoch 26, Batch 100 loss: 3.1031127214431762\n",
            "Epoch 26, Batch 120 loss: 3.1215929150581361\n",
            "Epoch 27, Batch 20 loss: 3.1523676395416258\n",
            "Epoch 27, Batch 40 loss: 3.1574690580368041\n",
            "Epoch 27, Batch 60 loss: 3.1442753672599792\n",
            "Epoch 27, Batch 80 loss: 3.1292746782302858\n",
            "Epoch 27, Batch 100 loss: 3.1042159557342530\n",
            "Epoch 27, Batch 120 loss: 3.1420922875404358\n",
            "Epoch 28, Batch 20 loss: 3.1675219297409059\n",
            "Epoch 28, Batch 40 loss: 3.1044529795646669\n",
            "Epoch 28, Batch 60 loss: 3.1566738724708556\n",
            "Epoch 28, Batch 80 loss: 3.1171149373054505\n",
            "Epoch 28, Batch 100 loss: 3.1293534874916076\n",
            "Epoch 28, Batch 120 loss: 3.1560043931007384\n",
            "Epoch 29, Batch 20 loss: 3.1100224733352659\n",
            "Epoch 29, Batch 40 loss: 3.1465287566184998\n",
            "Epoch 29, Batch 60 loss: 3.1136355757713319\n",
            "Epoch 29, Batch 80 loss: 3.1840506672859190\n",
            "Epoch 29, Batch 100 loss: 3.1041906356811522\n",
            "Epoch 29, Batch 120 loss: 3.1399013042449950\n",
            "Epoch 30, Batch 20 loss: 3.1400761961936952\n",
            "Epoch 30, Batch 40 loss: 3.1544205427169798\n",
            "Epoch 30, Batch 60 loss: 3.1189240694046019\n",
            "Epoch 30, Batch 80 loss: 3.1192129135131834\n",
            "Epoch 30, Batch 100 loss: 3.1471493005752564\n",
            "Epoch 30, Batch 120 loss: 3.1326305985450746\n",
            "Epoch 31, Batch 20 loss: 3.1201824307441712\n",
            "Epoch 31, Batch 40 loss: 3.1179434776306154\n",
            "Epoch 31, Batch 60 loss: 3.1530706763267515\n",
            "Epoch 31, Batch 80 loss: 3.0953020691871642\n",
            "Epoch 31, Batch 100 loss: 3.1600937366485597\n",
            "Epoch 31, Batch 120 loss: 3.1141095280647280\n",
            "Epoch 32, Batch 20 loss: 3.1095791339874266\n",
            "Epoch 32, Batch 40 loss: 3.1053608775138857\n",
            "Epoch 32, Batch 60 loss: 3.1342228770256044\n",
            "Epoch 32, Batch 80 loss: 3.1427550911903381\n",
            "Epoch 32, Batch 100 loss: 3.1311147570610047\n",
            "Epoch 32, Batch 120 loss: 3.1251748085021971\n",
            "Epoch 33, Batch 20 loss: 3.1284951210021972\n",
            "Epoch 33, Batch 40 loss: 3.1409603476524355\n",
            "Epoch 33, Batch 60 loss: 3.1431478738784788\n",
            "Epoch 33, Batch 80 loss: 3.1275794982910154\n",
            "Epoch 33, Batch 100 loss: 3.1154024481773375\n",
            "Epoch 33, Batch 120 loss: 3.0942208051681517\n",
            "Epoch 34, Batch 20 loss: 3.1446513652801515\n",
            "Epoch 34, Batch 40 loss: 3.1381843924522399\n",
            "Epoch 34, Batch 60 loss: 3.1154103875160217\n",
            "Epoch 34, Batch 80 loss: 3.1506194472312927\n",
            "Epoch 34, Batch 100 loss: 3.1099355459213256\n",
            "Epoch 34, Batch 120 loss: 3.0934895634651185\n",
            "Epoch 35, Batch 20 loss: 3.1143742203712463\n",
            "Epoch 35, Batch 40 loss: 3.1366290688514709\n",
            "Epoch 35, Batch 60 loss: 3.0884609699249266\n",
            "Epoch 35, Batch 80 loss: 3.1285434842109678\n",
            "Epoch 35, Batch 100 loss: 3.1270884871482849\n",
            "Epoch 35, Batch 120 loss: 3.1282368183135985\n",
            "Epoch 36, Batch 20 loss: 3.1369226336479188\n",
            "Epoch 36, Batch 40 loss: 3.1213896036148072\n",
            "Epoch 36, Batch 60 loss: 3.1310374498367310\n",
            "Epoch 36, Batch 80 loss: 3.1195796489715577\n",
            "Epoch 36, Batch 100 loss: 3.1215948462486267\n",
            "Epoch 36, Batch 120 loss: 3.1016777157783508\n",
            "Epoch 37, Batch 20 loss: 3.1163875341415403\n",
            "Epoch 37, Batch 40 loss: 3.1339572906494140\n",
            "Epoch 37, Batch 60 loss: 3.0957592368125915\n",
            "Epoch 37, Batch 80 loss: 3.1599386334419250\n",
            "Epoch 37, Batch 100 loss: 3.0883619427680968\n",
            "Epoch 37, Batch 120 loss: 3.1208734154701232\n",
            "Epoch 38, Batch 20 loss: 3.1232133030891420\n",
            "Epoch 38, Batch 40 loss: 3.1203681945800783\n",
            "Epoch 38, Batch 60 loss: 3.1358730912208559\n",
            "Epoch 38, Batch 80 loss: 3.1178841590881348\n",
            "Epoch 38, Batch 100 loss: 3.1160512447357176\n",
            "Epoch 38, Batch 120 loss: 3.1052016139030458\n",
            "Epoch 39, Batch 20 loss: 3.1422000050544741\n",
            "Epoch 39, Batch 40 loss: 3.0871196508407595\n",
            "Epoch 39, Batch 60 loss: 3.1179386854171751\n",
            "Epoch 39, Batch 80 loss: 3.1296411514282227\n",
            "Epoch 39, Batch 100 loss: 3.1086281061172487\n",
            "Epoch 39, Batch 120 loss: 3.1212348818778990\n",
            "Epoch 40, Batch 20 loss: 3.0844690799713135\n",
            "Epoch 40, Batch 40 loss: 3.1471911191940309\n",
            "Epoch 40, Batch 60 loss: 3.1223022580146789\n",
            "Epoch 40, Batch 80 loss: 3.1116263270378113\n",
            "Epoch 40, Batch 100 loss: 3.1083198070526121\n",
            "Epoch 40, Batch 120 loss: 3.1422498464584350\n",
            "Epoch 41, Batch 20 loss: 3.1186013936996462\n",
            "Epoch 41, Batch 40 loss: 3.1495747685432436\n",
            "Epoch 41, Batch 60 loss: 3.1100202441215514\n",
            "Epoch 41, Batch 80 loss: 3.1338324546813965\n",
            "Epoch 41, Batch 100 loss: 3.0763350963592528\n",
            "Epoch 41, Batch 120 loss: 3.1009808301925661\n",
            "Epoch 42, Batch 20 loss: 3.1097507953643797\n",
            "Epoch 42, Batch 40 loss: 3.1156897783279418\n",
            "Epoch 42, Batch 60 loss: 3.1040578007698061\n",
            "Epoch 42, Batch 80 loss: 3.1559850931167603\n",
            "Epoch 42, Batch 100 loss: 3.1073658347129820\n",
            "Epoch 42, Batch 120 loss: 3.1077543735504150\n",
            "Epoch 43, Batch 20 loss: 3.1266156911849974\n",
            "Epoch 43, Batch 40 loss: 3.1051516652107241\n",
            "Epoch 43, Batch 60 loss: 3.0938413858413698\n",
            "Epoch 43, Batch 80 loss: 3.1318415164947511\n",
            "Epoch 43, Batch 100 loss: 3.1019355773925783\n",
            "Epoch 43, Batch 120 loss: 3.1524635910987855\n",
            "Epoch 44, Batch 20 loss: 3.1072323441505434\n",
            "Epoch 44, Batch 40 loss: 3.1208155512809754\n",
            "Epoch 44, Batch 60 loss: 3.1554568290710447\n",
            "Epoch 44, Batch 80 loss: 3.1192183971405028\n",
            "Epoch 44, Batch 100 loss: 3.1124627590179443\n",
            "Epoch 44, Batch 120 loss: 3.0924564957618714\n",
            "Epoch 45, Batch 20 loss: 3.0892221927642822\n",
            "Epoch 45, Batch 40 loss: 3.1366045236587525\n",
            "Epoch 45, Batch 60 loss: 3.1355030536651611\n",
            "Epoch 45, Batch 80 loss: 3.0891592741012572\n",
            "Epoch 45, Batch 100 loss: 3.1247243762016295\n",
            "Epoch 45, Batch 120 loss: 3.1143893837928771\n",
            "Epoch 46, Batch 20 loss: 3.0837159872055055\n",
            "Epoch 46, Batch 40 loss: 3.1423700928688048\n",
            "Epoch 46, Batch 60 loss: 3.0987214207649232\n",
            "Epoch 46, Batch 80 loss: 3.0850466609001161\n",
            "Epoch 46, Batch 100 loss: 3.1201362490653990\n",
            "Epoch 46, Batch 120 loss: 3.1376200914382935\n",
            "Epoch 47, Batch 20 loss: 3.1636816382408144\n",
            "Epoch 47, Batch 40 loss: 3.1124562978744508\n",
            "Epoch 47, Batch 60 loss: 3.1008679747581480\n",
            "Epoch 47, Batch 80 loss: 3.0909426331520082\n",
            "Epoch 47, Batch 100 loss: 3.1250527501106262\n",
            "Epoch 47, Batch 120 loss: 3.1086613774299621\n",
            "Epoch 48, Batch 20 loss: 3.1097577452659606\n",
            "Epoch 48, Batch 40 loss: 3.1202751994132996\n",
            "Epoch 48, Batch 60 loss: 3.0893195986747743\n",
            "Epoch 48, Batch 80 loss: 3.0884672999382019\n",
            "Epoch 48, Batch 100 loss: 3.1633342742919921\n",
            "Epoch 48, Batch 120 loss: 3.1385046839714050\n",
            "Epoch 49, Batch 20 loss: 3.1273685693740845\n",
            "Epoch 49, Batch 40 loss: 3.1495334863662721\n",
            "Epoch 49, Batch 60 loss: 3.1005582332611086\n",
            "Epoch 49, Batch 80 loss: 3.0997416734695435\n",
            "Epoch 49, Batch 100 loss: 3.1022330045700075\n",
            "Epoch 49, Batch 120 loss: 3.0992642760276796\n",
            "Epoch 50, Batch 20 loss: 3.1166063070297243\n",
            "Epoch 50, Batch 40 loss: 3.1150378704071047\n",
            "Epoch 50, Batch 60 loss: 3.1060640096664427\n",
            "Epoch 50, Batch 80 loss: 3.1101391673088075\n",
            "Epoch 50, Batch 100 loss: 3.1084726452827454\n",
            "Epoch 50, Batch 120 loss: 3.1426387310028074\n",
            "Epoch 51, Batch 20 loss: 3.1059384942054749\n",
            "Epoch 51, Batch 40 loss: 3.0942056655883787\n",
            "Epoch 51, Batch 60 loss: 3.1507874727249146\n",
            "Epoch 51, Batch 80 loss: 3.1143810510635377\n",
            "Epoch 51, Batch 100 loss: 3.0677231788635253\n",
            "Epoch 51, Batch 120 loss: 3.1447290420532226\n",
            "Epoch 52, Batch 20 loss: 3.1098134040832521\n",
            "Epoch 52, Batch 40 loss: 3.1184829235076905\n",
            "Epoch 52, Batch 60 loss: 3.1162080168724060\n",
            "Epoch 52, Batch 80 loss: 3.1035403609275818\n",
            "Epoch 52, Batch 100 loss: 3.1039139270782470\n",
            "Epoch 52, Batch 120 loss: 3.1396460771560668\n",
            "Epoch 53, Batch 20 loss: 3.1376242995262147\n",
            "Epoch 53, Batch 40 loss: 3.1200773835182192\n",
            "Epoch 53, Batch 60 loss: 3.0958189129829408\n",
            "Epoch 53, Batch 80 loss: 3.1227429747581481\n",
            "Epoch 53, Batch 100 loss: 3.1394847989082337\n",
            "Epoch 53, Batch 120 loss: 3.0802008748054504\n",
            "Epoch 54, Batch 20 loss: 3.1400305032730103\n",
            "Epoch 54, Batch 40 loss: 3.1353792309761048\n",
            "Epoch 54, Batch 60 loss: 3.1268301725387575\n",
            "Epoch 54, Batch 80 loss: 3.1040936708450317\n",
            "Epoch 54, Batch 100 loss: 3.0884816408157350\n",
            "Epoch 54, Batch 120 loss: 3.1218970179557801\n",
            "Epoch 55, Batch 20 loss: 3.1041749596595762\n",
            "Epoch 55, Batch 40 loss: 3.1425405144691467\n",
            "Epoch 55, Batch 60 loss: 3.1261911392211914\n",
            "Epoch 55, Batch 80 loss: 3.1127064824104309\n",
            "Epoch 55, Batch 100 loss: 3.1014906764030457\n",
            "Epoch 55, Batch 120 loss: 3.1023489356040956\n",
            "Epoch 56, Batch 20 loss: 3.0888577222824098\n",
            "Epoch 56, Batch 40 loss: 3.1222697257995606\n",
            "Epoch 56, Batch 60 loss: 3.1385433673858643\n",
            "Epoch 56, Batch 80 loss: 3.1330477952957154\n",
            "Epoch 56, Batch 100 loss: 3.1034509897232057\n",
            "Epoch 56, Batch 120 loss: 3.1082386374473572\n",
            "Epoch 57, Batch 20 loss: 3.1252778530120850\n",
            "Epoch 57, Batch 40 loss: 3.1052053928375245\n",
            "Epoch 57, Batch 60 loss: 3.1017393350601195\n",
            "Epoch 57, Batch 80 loss: 3.1041884303092955\n",
            "Epoch 57, Batch 100 loss: 3.1315387129783629\n",
            "Epoch 57, Batch 120 loss: 3.1095289587974548\n",
            "Epoch 58, Batch 20 loss: 3.1314187526702879\n",
            "Epoch 58, Batch 40 loss: 3.0899598598480225\n",
            "Epoch 58, Batch 60 loss: 3.1309568166732786\n",
            "Epoch 58, Batch 80 loss: 3.1046378970146180\n",
            "Epoch 58, Batch 100 loss: 3.1074432492256165\n",
            "Epoch 58, Batch 120 loss: 3.1028663396835325\n",
            "Epoch 59, Batch 20 loss: 3.0942688941955567\n",
            "Epoch 59, Batch 40 loss: 3.1234008908271789\n",
            "Epoch 59, Batch 60 loss: 3.1033717393875122\n",
            "Epoch 59, Batch 80 loss: 3.1105637907981873\n",
            "Epoch 59, Batch 100 loss: 3.1142804503440855\n",
            "Epoch 59, Batch 120 loss: 3.1479720115661620\n",
            "Epoch 60, Batch 20 loss: 3.1053638696670531\n",
            "Epoch 60, Batch 40 loss: 3.0866970300674437\n",
            "Epoch 60, Batch 60 loss: 3.0901454806327822\n",
            "Epoch 60, Batch 80 loss: 3.1692055106163024\n",
            "Epoch 60, Batch 100 loss: 3.1267820835113525\n",
            "Epoch 60, Batch 120 loss: 3.0865319728851319\n",
            "Epoch 61, Batch 20 loss: 3.1002237081527708\n",
            "Epoch 61, Batch 40 loss: 3.0982151269912719\n",
            "Epoch 61, Batch 60 loss: 3.1005092382431032\n",
            "Epoch 61, Batch 80 loss: 3.1271933197975157\n",
            "Epoch 61, Batch 100 loss: 3.1354602098464968\n",
            "Epoch 61, Batch 120 loss: 3.1162230849266050\n",
            "Epoch 62, Batch 20 loss: 3.1265504956245422\n",
            "Epoch 62, Batch 40 loss: 3.0980659842491152\n",
            "Epoch 62, Batch 60 loss: 3.1120265007019041\n",
            "Epoch 62, Batch 80 loss: 3.1185527324676512\n",
            "Epoch 62, Batch 100 loss: 3.1185927748680116\n",
            "Epoch 62, Batch 120 loss: 3.1223820090293883\n",
            "Epoch 63, Batch 20 loss: 3.1389850139617921\n",
            "Epoch 63, Batch 40 loss: 3.1189200758934019\n",
            "Epoch 63, Batch 60 loss: 3.1099884867668153\n",
            "Epoch 63, Batch 80 loss: 3.1165354967117311\n",
            "Epoch 63, Batch 100 loss: 3.1328735232353209\n",
            "Epoch 63, Batch 120 loss: 3.0968885421752930\n",
            "Epoch 64, Batch 20 loss: 3.1304817318916323\n",
            "Epoch 64, Batch 40 loss: 3.0947292208671571\n",
            "Epoch 64, Batch 60 loss: 3.1270194172859194\n",
            "Epoch 64, Batch 80 loss: 3.1087146162986756\n",
            "Epoch 64, Batch 100 loss: 3.0976652383804320\n",
            "Epoch 64, Batch 120 loss: 3.1124003767967223\n",
            "Epoch 65, Batch 20 loss: 3.1417587041854858\n",
            "Epoch 65, Batch 40 loss: 3.0939352035522463\n",
            "Epoch 65, Batch 60 loss: 3.1132445216178892\n",
            "Epoch 65, Batch 80 loss: 3.1095650792121887\n",
            "Epoch 65, Batch 100 loss: 3.1004376173019410\n",
            "Epoch 65, Batch 120 loss: 3.1208062648773192\n",
            "Epoch 66, Batch 20 loss: 3.1205580711364744\n",
            "Epoch 66, Batch 40 loss: 3.1195962667465209\n",
            "Epoch 66, Batch 60 loss: 3.0988229990005491\n",
            "Epoch 66, Batch 80 loss: 3.1550791144371031\n",
            "Epoch 66, Batch 100 loss: 3.1227512121200562\n",
            "Epoch 66, Batch 120 loss: 3.0686659455299377\n",
            "Epoch 67, Batch 20 loss: 3.1111906886100771\n",
            "Epoch 67, Batch 40 loss: 3.1180935144424438\n",
            "Epoch 67, Batch 60 loss: 3.1334766387939452\n",
            "Epoch 67, Batch 80 loss: 3.1148319005966187\n",
            "Epoch 67, Batch 100 loss: 3.1134141921997069\n",
            "Epoch 67, Batch 120 loss: 3.1086752891540526\n",
            "Epoch 68, Batch 20 loss: 3.1311380624771119\n",
            "Epoch 68, Batch 40 loss: 3.0892928123474119\n",
            "Epoch 68, Batch 60 loss: 3.1425322175025938\n",
            "Epoch 68, Batch 80 loss: 3.0978829979896547\n",
            "Epoch 68, Batch 100 loss: 3.1187727570533754\n",
            "Epoch 68, Batch 120 loss: 3.1072828888893129\n",
            "Epoch 69, Batch 20 loss: 3.0969909071922301\n",
            "Epoch 69, Batch 40 loss: 3.1062971353530884\n",
            "Epoch 69, Batch 60 loss: 3.1785980939865111\n",
            "Epoch 69, Batch 80 loss: 3.0860798358917236\n",
            "Epoch 69, Batch 100 loss: 3.1134830832481386\n",
            "Epoch 69, Batch 120 loss: 3.1208949208259584\n",
            "Epoch 70, Batch 20 loss: 3.1088654637336730\n",
            "Epoch 70, Batch 40 loss: 3.1027301549911499\n",
            "Epoch 70, Batch 60 loss: 3.1105805754661562\n",
            "Epoch 70, Batch 80 loss: 3.1495085954666138\n",
            "Epoch 70, Batch 100 loss: 3.0880465745925902\n",
            "Epoch 70, Batch 120 loss: 3.1237296223640443\n",
            "Epoch 71, Batch 20 loss: 3.1037852287292482\n",
            "Epoch 71, Batch 40 loss: 3.0956778287887574\n",
            "Epoch 71, Batch 60 loss: 3.1077747344970703\n",
            "Epoch 71, Batch 80 loss: 3.1245591759681703\n",
            "Epoch 71, Batch 100 loss: 3.1584912419319151\n",
            "Epoch 71, Batch 120 loss: 3.1114519357681276\n",
            "Epoch 72, Batch 20 loss: 3.1367934584617614\n",
            "Epoch 72, Batch 40 loss: 3.0935282826423647\n",
            "Epoch 72, Batch 60 loss: 3.0841560125350953\n",
            "Epoch 72, Batch 80 loss: 3.1355156540870666\n",
            "Epoch 72, Batch 100 loss: 3.1086215376853943\n",
            "Epoch 72, Batch 120 loss: 3.1279977917671205\n",
            "Epoch 73, Batch 20 loss: 3.1335935592651367\n",
            "Epoch 73, Batch 40 loss: 3.0914380788803100\n",
            "Epoch 73, Batch 60 loss: 3.1041912078857421\n",
            "Epoch 73, Batch 80 loss: 3.1164561033248903\n",
            "Epoch 73, Batch 100 loss: 3.1019327402114869\n",
            "Epoch 73, Batch 120 loss: 3.1227737069129944\n",
            "Epoch 74, Batch 20 loss: 3.1419756293296812\n",
            "Epoch 74, Batch 40 loss: 3.1378014087677002\n",
            "Epoch 74, Batch 60 loss: 3.0561877131462096\n",
            "Epoch 74, Batch 80 loss: 3.1203448653221129\n",
            "Epoch 74, Batch 100 loss: 3.1221696734428406\n",
            "Epoch 74, Batch 120 loss: 3.1257080793380738\n",
            "Epoch 75, Batch 20 loss: 3.1542693614959716\n",
            "Epoch 75, Batch 40 loss: 3.1284868836402895\n",
            "Epoch 75, Batch 60 loss: 3.1070912361145018\n",
            "Epoch 75, Batch 80 loss: 3.0878332853317261\n",
            "Epoch 75, Batch 100 loss: 3.1075085282325743\n",
            "Epoch 75, Batch 120 loss: 3.1258489012718202\n",
            "Epoch 76, Batch 20 loss: 3.1199009776115418\n",
            "Epoch 76, Batch 40 loss: 3.1203415155410767\n",
            "Epoch 76, Batch 60 loss: 3.1428105115890501\n",
            "Epoch 76, Batch 80 loss: 3.1161059856414797\n",
            "Epoch 76, Batch 100 loss: 3.0871578097343444\n",
            "Epoch 76, Batch 120 loss: 3.1286271214485168\n",
            "Epoch 77, Batch 20 loss: 3.1470438241958618\n",
            "Epoch 77, Batch 40 loss: 3.0797773241996764\n",
            "Epoch 77, Batch 60 loss: 3.0796683192253114\n",
            "Epoch 77, Batch 80 loss: 3.1317240953445435\n",
            "Epoch 77, Batch 100 loss: 3.1062267899513243\n",
            "Epoch 77, Batch 120 loss: 3.1290588736534120\n",
            "Epoch 78, Batch 20 loss: 3.1291131973266602\n",
            "Epoch 78, Batch 40 loss: 3.1220362424850463\n",
            "Epoch 78, Batch 60 loss: 3.0836363315582274\n",
            "Epoch 78, Batch 80 loss: 3.1201466321945190\n",
            "Epoch 78, Batch 100 loss: 3.1376785397529603\n",
            "Epoch 78, Batch 120 loss: 3.1145382046699526\n",
            "Epoch 79, Batch 20 loss: 3.0938467025756835\n",
            "Epoch 79, Batch 40 loss: 3.1719504356384278\n",
            "Epoch 79, Batch 60 loss: 3.0589231729507445\n",
            "Epoch 79, Batch 80 loss: 3.1457212448120115\n",
            "Epoch 79, Batch 100 loss: 3.0963497638702391\n",
            "Epoch 79, Batch 120 loss: 3.1041676044464110\n",
            "Epoch 80, Batch 20 loss: 3.1188657164573668\n",
            "Epoch 80, Batch 40 loss: 3.1118070960044859\n",
            "Epoch 80, Batch 60 loss: 3.1117379307746886\n",
            "Epoch 80, Batch 80 loss: 3.1521475553512572\n",
            "Epoch 80, Batch 100 loss: 3.0910989999771119\n",
            "Epoch 80, Batch 120 loss: 3.1114843487739563\n",
            "Epoch 81, Batch 20 loss: 3.1111161470413209\n",
            "Epoch 81, Batch 40 loss: 3.1516142368316649\n",
            "Epoch 81, Batch 60 loss: 3.1282718658447264\n",
            "Epoch 81, Batch 80 loss: 3.1069195508956908\n",
            "Epoch 81, Batch 100 loss: 3.0842514634132385\n",
            "Epoch 81, Batch 120 loss: 3.1209200501441954\n",
            "Epoch 82, Batch 20 loss: 3.1046704411506654\n",
            "Epoch 82, Batch 40 loss: 3.1403795242309571\n",
            "Epoch 82, Batch 60 loss: 3.1180168986320496\n",
            "Epoch 82, Batch 80 loss: 3.1248728632926941\n",
            "Epoch 82, Batch 100 loss: 3.1026859760284422\n",
            "Epoch 82, Batch 120 loss: 3.1194540143012999\n",
            "Epoch 83, Batch 20 loss: 3.1272418260574342\n",
            "Epoch 83, Batch 40 loss: 3.1150398850440979\n",
            "Epoch 83, Batch 60 loss: 3.0923768281936646\n",
            "Epoch 83, Batch 80 loss: 3.1598370671272278\n",
            "Epoch 83, Batch 100 loss: 3.0936699748039245\n",
            "Epoch 83, Batch 120 loss: 3.1096595764160155\n",
            "Epoch 84, Batch 20 loss: 3.1076363325119019\n",
            "Epoch 84, Batch 40 loss: 3.0936358809471129\n",
            "Epoch 84, Batch 60 loss: 3.1437450766563417\n",
            "Epoch 84, Batch 80 loss: 3.1018418431282044\n",
            "Epoch 84, Batch 100 loss: 3.1426553487777711\n",
            "Epoch 84, Batch 120 loss: 3.1158426284790037\n",
            "Epoch 85, Batch 20 loss: 3.1138815283775330\n",
            "Epoch 85, Batch 40 loss: 3.1117236495018004\n",
            "Epoch 85, Batch 60 loss: 3.1097321867942811\n",
            "Epoch 85, Batch 80 loss: 3.1102596521377563\n",
            "Epoch 85, Batch 100 loss: 3.0830177664756775\n",
            "Epoch 85, Batch 120 loss: 3.1341928958892824\n",
            "Epoch 86, Batch 20 loss: 3.0965377092361450\n",
            "Epoch 86, Batch 40 loss: 3.1242807507514954\n",
            "Epoch 86, Batch 60 loss: 3.1098424077033995\n",
            "Epoch 86, Batch 80 loss: 3.1117475628852844\n",
            "Epoch 86, Batch 100 loss: 3.0938511610031130\n",
            "Epoch 86, Batch 120 loss: 3.1574450492858888\n",
            "Epoch 87, Batch 20 loss: 3.1290168404579162\n",
            "Epoch 87, Batch 40 loss: 3.0856190085411073\n",
            "Epoch 87, Batch 60 loss: 3.1299242377281189\n",
            "Epoch 87, Batch 80 loss: 3.1237117409706117\n",
            "Epoch 87, Batch 100 loss: 3.1284403920173647\n",
            "Epoch 87, Batch 120 loss: 3.0868474364280702\n",
            "Epoch 88, Batch 20 loss: 3.1162097811698914\n",
            "Epoch 88, Batch 40 loss: 3.1107629060745241\n",
            "Epoch 88, Batch 60 loss: 3.1580208063125612\n",
            "Epoch 88, Batch 80 loss: 3.1028618216514587\n",
            "Epoch 88, Batch 100 loss: 3.0815556526184080\n",
            "Epoch 88, Batch 120 loss: 3.1256158709526063\n",
            "Epoch 89, Batch 20 loss: 3.0780231595039367\n",
            "Epoch 89, Batch 40 loss: 3.1372631430625915\n",
            "Epoch 89, Batch 60 loss: 3.1084073066711424\n",
            "Epoch 89, Batch 80 loss: 3.1179540872573854\n",
            "Epoch 89, Batch 100 loss: 3.1139394044876099\n",
            "Epoch 89, Batch 120 loss: 3.1320506215095518\n",
            "Epoch 90, Batch 20 loss: 3.1378763318061829\n",
            "Epoch 90, Batch 40 loss: 3.1108475685119630\n",
            "Epoch 90, Batch 60 loss: 3.1004237890243531\n",
            "Epoch 90, Batch 80 loss: 3.0879008531570435\n",
            "Epoch 90, Batch 100 loss: 3.1018200397491453\n",
            "Epoch 90, Batch 120 loss: 3.1325837492942812\n",
            "Epoch 91, Batch 20 loss: 3.0640615463256835\n",
            "Epoch 91, Batch 40 loss: 3.1215050816535950\n",
            "Epoch 91, Batch 60 loss: 3.0931015968322755\n",
            "Epoch 91, Batch 80 loss: 3.1614817261695860\n",
            "Epoch 91, Batch 100 loss: 3.1385403037071229\n",
            "Epoch 91, Batch 120 loss: 3.1101422429084780\n",
            "Epoch 92, Batch 20 loss: 3.1188053488731384\n",
            "Epoch 92, Batch 40 loss: 3.1086005091667177\n",
            "Epoch 92, Batch 60 loss: 3.1154696106910706\n",
            "Epoch 92, Batch 80 loss: 3.1192598342895508\n",
            "Epoch 92, Batch 100 loss: 3.1054150223731996\n",
            "Epoch 92, Batch 120 loss: 3.1004213571548460\n",
            "Epoch 93, Batch 20 loss: 3.1463864207267762\n",
            "Epoch 93, Batch 40 loss: 3.0956322669982912\n",
            "Epoch 93, Batch 60 loss: 3.1225363254547118\n",
            "Epoch 93, Batch 80 loss: 3.1159230232238770\n",
            "Epoch 93, Batch 100 loss: 3.1051813364028931\n",
            "Epoch 93, Batch 120 loss: 3.1004727125167846\n",
            "Epoch 94, Batch 20 loss: 3.1247166991233826\n",
            "Epoch 94, Batch 40 loss: 3.1081343889236450\n",
            "Epoch 94, Batch 60 loss: 3.1006414890289307\n",
            "Epoch 94, Batch 80 loss: 3.0958626866340637\n",
            "Epoch 94, Batch 100 loss: 3.1029101967811585\n",
            "Epoch 94, Batch 120 loss: 3.1267663717269896\n",
            "Epoch 95, Batch 20 loss: 3.1509713172912597\n",
            "Epoch 95, Batch 40 loss: 3.0804940462112427\n",
            "Epoch 95, Batch 60 loss: 3.1215175509452822\n",
            "Epoch 95, Batch 80 loss: 3.1089370727539061\n",
            "Epoch 95, Batch 100 loss: 3.1053163766860963\n",
            "Epoch 95, Batch 120 loss: 3.1419701933860780\n",
            "Epoch 96, Batch 20 loss: 3.1365767240524294\n",
            "Epoch 96, Batch 40 loss: 3.1073272347450258\n",
            "Epoch 96, Batch 60 loss: 3.1238431572914123\n",
            "Epoch 96, Batch 80 loss: 3.1156744599342345\n",
            "Epoch 96, Batch 100 loss: 3.0624686837196351\n",
            "Epoch 96, Batch 120 loss: 3.1501297116279603\n",
            "Epoch 97, Batch 20 loss: 3.1295290946960450\n",
            "Epoch 97, Batch 40 loss: 3.0967207074165346\n",
            "Epoch 97, Batch 60 loss: 3.1194154620170593\n",
            "Epoch 97, Batch 80 loss: 3.1094308614730837\n",
            "Epoch 97, Batch 100 loss: 3.1160825252532960\n",
            "Epoch 97, Batch 120 loss: 3.1099535942077638\n",
            "Epoch 98, Batch 20 loss: 3.1324806571006776\n",
            "Epoch 98, Batch 40 loss: 3.1132163643836974\n",
            "Epoch 98, Batch 60 loss: 3.0893704175949095\n",
            "Epoch 98, Batch 80 loss: 3.1175593376159667\n",
            "Epoch 98, Batch 100 loss: 3.1125291585922241\n",
            "Epoch 98, Batch 120 loss: 3.1065166234970092\n",
            "Epoch 99, Batch 20 loss: 3.1310719251632690\n",
            "Epoch 99, Batch 40 loss: 3.0731447219848631\n",
            "Epoch 99, Batch 60 loss: 3.1051338195800779\n",
            "Epoch 99, Batch 80 loss: 3.1134608387947083\n",
            "Epoch 99, Batch 100 loss: 3.1757996201515200\n",
            "Epoch 99, Batch 120 loss: 3.0899800539016722\n",
            "Epoch 100, Batch 20 loss: 3.1195414543151854\n",
            "Epoch 100, Batch 40 loss: 3.0864781141281128\n",
            "Epoch 100, Batch 60 loss: 3.1371197581291197\n",
            "Epoch 100, Batch 80 loss: 3.1203711032867432\n",
            "Epoch 100, Batch 100 loss: 3.1177684783935549\n",
            "Epoch 100, Batch 120 loss: 3.1001133799552916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "CfA7ZjLR1BZP",
        "outputId": "d4bb897e-8048-4c04-9671-c7478b1eebe1"
      },
      "source": [
        "plt.plot(train_losses)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItLyF9dQyJFL"
      },
      "source": [
        "Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGki98W3SoEW"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNavYOynyE6d",
        "outputId": "1f8a80ee-a5c8-4b71-e1ba-83a72cc0665d"
      },
      "source": [
        "# The test_model function is from model_testing python file\n",
        "test_loss, class_correct, class_total, labels, predictions = test_model(classes, glyphnet, test_loader, criterion)\n",
        "\n",
        "# Test accuracy for each hieroglyph\n",
        "for i in range(len(classes)):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (classes[i], 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "# Total Test accuracy\n",
        "print(\"\\nAccuracy: {:.3%}\".format(accuracy_score(labels, predictions)))\n",
        "print(\"\\nPrecision: {:.3%}\".format(precision_score(labels, predictions, average = 'weighted')))\n",
        "print(\"\\nRecall: {:.3%}\".format(recall_score(labels, predictions, average = 'weighted')))\n",
        "print(\"\\nF1-score: {:.3%}\".format(f1_score(labels, predictions, average = 'weighted')))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 3.661894\n",
            "\n",
            "Test Accuracy of    D2:  0% ( 0/ 5)\n",
            "Test Accuracy of   D21: 100% (36/36)\n",
            "Test Accuracy of   D36:  0% ( 0/12)\n",
            "Test Accuracy of    D4:  0% ( 0/ 8)\n",
            "Test Accuracy of   D46:  0% ( 0/11)\n",
            "Test Accuracy of   D58:  0% ( 0/ 8)\n",
            "Test Accuracy of   E23:  0% ( 0/ 2)\n",
            "Test Accuracy of   E34:  0% ( 0/25)\n",
            "Test Accuracy of   F31:  0% ( 0/ 2)\n",
            "Test Accuracy of   F35:  0% ( 0/ 1)\n",
            "Test Accuracy of    G1:  0% ( 0/ 7)\n",
            "Test Accuracy of   G17:  0% ( 0/39)\n",
            "Test Accuracy of   G43:  0% ( 0/40)\n",
            "Test Accuracy of   I10:  0% ( 0/ 9)\n",
            "Test Accuracy of    I9:  0% ( 0/30)\n",
            "Test Accuracy of   M17:  0% ( 0/73)\n",
            "Test Accuracy of   M23:  0% ( 0/ 8)\n",
            "Test Accuracy of   N35:  0% ( 0/90)\n",
            "Test Accuracy of    O1:  0% ( 0/ 4)\n",
            "Test Accuracy of   O34:  0% ( 0/ 4)\n",
            "Test Accuracy of    O4:  0% ( 0/ 2)\n",
            "Test Accuracy of   O49:  0% ( 0/ 3)\n",
            "Test Accuracy of    Q1:  0% ( 0/ 4)\n",
            "Test Accuracy of    Q3:  0% ( 0/16)\n",
            "Test Accuracy of    R4:  0% ( 0/ 1)\n",
            "Test Accuracy of    R8:  0% ( 0/14)\n",
            "Test Accuracy of   S29:  0% ( 0/53)\n",
            "Test Accuracy of   S34:  0% ( 0/ 2)\n",
            "Test Accuracy of    U7:  0% ( 0/ 1)\n",
            "Test Accuracy of   V13:  0% ( 0/16)\n",
            "Test Accuracy of   V28:  0% ( 0/ 8)\n",
            "Test Accuracy of   V30:  0% ( 0/ 2)\n",
            "Test Accuracy of   V31:  0% ( 0/27)\n",
            "Test Accuracy of   W11:  0% ( 0/ 1)\n",
            "Test Accuracy of   W24:  0% ( 0/ 8)\n",
            "Test Accuracy of    X1:  0% ( 0/47)\n",
            "Test Accuracy of    X8:  0% ( 0/ 1)\n",
            "Test Accuracy of    Y1:  0% ( 0/ 1)\n",
            "Test Accuracy of    Y5:  0% ( 0/ 2)\n",
            "Test Accuracy of    Z1:  0% ( 0/10)\n",
            "\n",
            "Accuracy: 5.687%\n",
            "\n",
            "Precision: 0.323%\n",
            "\n",
            "Recall: 5.687%\n",
            "\n",
            "F1-score: 0.612%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}