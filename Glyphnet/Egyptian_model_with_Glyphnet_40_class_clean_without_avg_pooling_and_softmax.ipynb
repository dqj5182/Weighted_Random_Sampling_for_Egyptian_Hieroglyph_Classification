{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Egyptian_model_with_Glyphnet_40_class_clean_without_avg_pooling_and_softmax.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iEiGSblxJzW"
      },
      "source": [
        "import os, os.path\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from pathlib import Path\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd6KU8VMz6J3"
      },
      "source": [
        "Module: Load_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lWX0BW_w_kC"
      },
      "source": [
        "def load_data(hieroglyph_directory_path, batch_size=20, num_workers=0):\n",
        "    train_dir = os.path.join(hieroglyph_directory_path, 'train/')\n",
        "    test_dir = os.path.join(hieroglyph_directory_path, 'test/')\n",
        "\n",
        "    classes = []\n",
        "\n",
        "    for filename in os.listdir(train_dir):\n",
        "        if filename == '.DS_Store':\n",
        "            pass\n",
        "        else:\n",
        "            classes.append(filename)\n",
        "\n",
        "    classes.sort()\n",
        "\n",
        "    # print(\"Our classes:\", classes)\n",
        "    # print(len(classes))\n",
        "\n",
        "    data_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                                transforms.RandomApply([transforms.RandomHorizontalFlip()]),\n",
        "                                                transforms.RandomRotation(degrees=(-10, 10)),\n",
        "                                                transforms.RandomAffine(degrees=0, translate=(.1, .1)),\n",
        "                                                transforms.RandomApply([transforms.ColorJitter(brightness=(1, 1.2),\n",
        "                                                                                                contrast=(1, 1.5),\n",
        "                                                                                                saturation=(1, 1.5),\n",
        "                                                                                                hue=(0, 0.5))]),\n",
        "                                                transforms.RandomErasing(p=0.5, scale=(0.05, 0.05), ratio=(0.3, 3.3), value=0,\n",
        "                                                                          inplace=False),\n",
        "                                                transforms.Resize((100, 100)),\n",
        "                                                transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "    train_data = datasets.ImageFolder(train_dir, transform=data_transform)\n",
        "    test_data = datasets.ImageFolder(test_dir, transform=data_transform)\n",
        "\n",
        "    # print('Num training images: ', len(train_data))\n",
        "    # print('Num test images: ', len(test_data))\n",
        "\n",
        "    # prepare data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "                                               num_workers=num_workers, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
        "                                              num_workers=num_workers, shuffle=True)\n",
        "\n",
        "    return train_loader, test_loader, classes"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4sXUrPPz7iF"
      },
      "source": [
        "Module: Train_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2N_rnKLxN5c"
      },
      "source": [
        "def train_model(train_loader, optimizer, conv_net, criterion, my_lr_scheduler, n_epochs):\n",
        "    # track training loss over time\n",
        "    losses = []\n",
        "    \n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "        # keep track of training and validation loss\n",
        "        train_loss = 0.0\n",
        "\n",
        "        # model by default is set to train\n",
        "        for batch_i, (data, target) in enumerate(train_loader):\n",
        "            # move tensors to GPU if CUDA is available\n",
        "            if train_on_gpu:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = conv_net(data)\n",
        "\n",
        "            # calculate the batch loss\n",
        "            loss = criterion(output, target)\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "            # update training loss\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            my_lr_scheduler.step()\n",
        "\n",
        "            if batch_i % 20 == 19:  # print training loss every specified number of mini-batches\n",
        "                print('Epoch %d, Batch %d loss: %.16f' %\n",
        "                    (epoch, batch_i + 1, train_loss / 20))\n",
        "                losses.append(train_loss / 20)\n",
        "                train_loss = 0.0\n",
        "        \n",
        "        \n",
        "\n",
        "    return conv_net, losses"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Rrxo5RDz96U"
      },
      "source": [
        "Module: Test_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay-HlMJnxU9v"
      },
      "source": [
        "def test_model(classes, conv_net, test_loader, criterion):\n",
        "    # track test loss\n",
        "    test_loss = 0.0\n",
        "    class_correct = list(0. for i in range(len(classes)))\n",
        "    class_total = list(0. for i in range(len(classes)))\n",
        "\n",
        "    conv_net.eval()  # eval mode\n",
        "\n",
        "    labels = []\n",
        "    predictions = []\n",
        "    # iterate over test data\n",
        "    for data, target in test_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = conv_net(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # update  test loss\n",
        "        test_loss += loss.item() * data.size(0)\n",
        "        # convert output probabilities to predicted class\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "        # compare predictions to true label\n",
        "        correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "        # calculate test accuracy for each object class\n",
        "        for i in range(len(target.data)):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "        \n",
        "        # Will be used for calculating Recall, Precision, and F1-score\n",
        "        labels.extend(target.data.view_as(pred).tolist())\n",
        "        predictions.extend(pred.tolist())\n",
        "\n",
        "\n",
        "    # calculate avg test loss\n",
        "    test_loss = test_loss / len(test_loader.dataset)\n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "    return test_loss, class_correct, class_total, labels, predictions"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHh3PXE80Asa"
      },
      "source": [
        "Check whether CUDA is available (Change runtime type if not)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9TYPQH7x4zw",
        "outputId": "c16c83c6-e1c8-47e0-b156-22dc4dfb7e86"
      },
      "source": [
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lYw8EKVx8Q7"
      },
      "source": [
        "Load Hieroglyph Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laLvoRy1yewl",
        "outputId": "5b393cc0-5606-472f-dcfd-07a965e8c491"
      },
      "source": [
        "# Connecting and Mounting to the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odaoZt9GyfUt"
      },
      "source": [
        "data_dir = '/content/drive/MyDrive/EgyptianHieroglyphDataset_Original_Clean/'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF22MAnmyiC1",
        "outputId": "f47149a8-3c7c-451d-db79-0fc60499fdd7"
      },
      "source": [
        "hieroglyph_for_train = []\n",
        "file_count_list = []\n",
        "\n",
        "for name in os.listdir('/content/drive/MyDrive/EgyptianHieroglyphDataset_Original_Clean/train/'):\n",
        "  path, dirs, files = next(os.walk(\"/content/drive/MyDrive/EgyptianHieroglyphDataset_Original_Clean/train/\"+name))\n",
        "  file_count = len(files)\n",
        "  print(name, file_count)\n",
        "  file_count_list.append(file_count)\n",
        "  hieroglyph_for_train.append(name)\n",
        "\n",
        "hieroglyph_dict = dict(zip(hieroglyph_for_train, file_count_list))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y5 6\n",
            "I9 116\n",
            "O34 15\n",
            "V13 63\n",
            "U7 3\n",
            "D46 40\n",
            "E34 97\n",
            "G1 28\n",
            "V31 106\n",
            "S34 8\n",
            "D36 47\n",
            "Q1 13\n",
            "V30 6\n",
            "F35 2\n",
            "O4 11\n",
            "M23 30\n",
            "S29 212\n",
            "R8 53\n",
            "W11 4\n",
            "X1 185\n",
            "Y1 1\n",
            "D21 146\n",
            "D2 19\n",
            "E23 8\n",
            "X8 4\n",
            "Z1 39\n",
            "D4 29\n",
            "V28 28\n",
            "I10 32\n",
            "O1 16\n",
            "M17 291\n",
            "F31 6\n",
            "G43 157\n",
            "D58 28\n",
            "O49 10\n",
            "G17 156\n",
            "R4 2\n",
            "Q3 61\n",
            "W24 31\n",
            "N35 358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeQQ7PjZzKd9"
      },
      "source": [
        "Number of images for each hieroglyph "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "W6IzWR2RyjxM",
        "outputId": "349be75a-cac4-4af4-956c-bcf23676fc13"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\"Hieroglyph\":hieroglyph_for_train, \"Count\":file_count_list})\n",
        "\n",
        "df_sorted= df.sort_values('Count',ascending=False)\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "# make bar plot with matplotlib\n",
        "plt.bar('Hieroglyph', 'Count',data=df_sorted)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 40 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3oOm8rSx7tp"
      },
      "source": [
        "# Number of images processed in a single training\n",
        "batch_size = 20\n",
        "num_workers = 0\n",
        "\n",
        "# The load_data function is from hieroglyph_data_preparation python file\n",
        "train_loader, test_loader, classes = load_data(data_dir)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJm8sGiBx_q5"
      },
      "source": [
        "ResNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii1UaV8yPvW_"
      },
      "source": [
        "class SeparableConv2d(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,kernel_size=3,stride=1,padding=0,dilation=1,bias=False):\n",
        "        super(SeparableConv2d,self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n",
        "        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hnE4jAAPbnX"
      },
      "source": [
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "oXpu43oPMyor",
        "outputId": "026ce5af-56a7-4072-eca5-37b52ca5b6e2"
      },
      "source": [
        "'''\n",
        "class Glyphnet(torch.nn.Module):   \n",
        "    def __init__(self):\n",
        "        super(Glyphnet, self).__init__()\n",
        "\n",
        "        self.conv1 = torch.nn.Conv2d(3, 64, kernel_size = 3, stride = 1, padding = 0)\n",
        "        self.maxpool1 = MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = torch.nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 0)\n",
        "        self.maxpool2 = MaxPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "        self.fc1 = Linear(23 * 23 * 64, 40)\n",
        "\n",
        "        #self.conv1 = torch.nn.Conv2d(3, 64, kernel_size = 3, stride = 1, padding = 0)\n",
        "        #self.conv2 = torch.nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 0)\n",
        "        #self.maxpool = torch.nn.MaxPool2d(kernel_size = 3, stride = 1, padding = 0)\n",
        "        #self.avgpool = torch.nn.AvgPool2d(kernel_size=66)\n",
        "\n",
        "        #self.bn1 = torch.nn.BatchNorm2d(64)\n",
        "        #self.bn2 = torch.nn.BatchNorm2d(128)\n",
        "        #self.bn3 = torch.nn.BatchNorm2d(256)\n",
        "        #self.bn4 = torch.nn.BatchNorm2d(512)\n",
        "\n",
        "        #self.separableconv1 = SeparableConv2d(64,128)\n",
        "        #self.separableconv2 = SeparableConv2d(128,128)\n",
        "        #self.separableconv3 = SeparableConv2d(128,256)\n",
        "        #self.separableconv4 = SeparableConv2d(256,256)\n",
        "        #self.separableconv5 = SeparableConv2d(256,512)\n",
        "\n",
        "        #self.drop_layer = nn.Dropout(p=0.15)\n",
        "        #self.fc1 = torch.nn.Linear(512, 40) # Should change 64 part\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    # Defining the forward pass    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        # x = self.softmax(x) # Not a good idea (the model is trained really slow)\n",
        "\n",
        "        return x\n",
        "'''"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nclass Glyphnet(torch.nn.Module):   \\n    def __init__(self):\\n        super(Glyphnet, self).__init__()\\n\\n        self.conv1 = torch.nn.Conv2d(3, 64, kernel_size = 3, stride = 1, padding = 0)\\n        self.maxpool1 = MaxPool2d(kernel_size=2, stride=2)\\n        self.conv2 = torch.nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 0)\\n        self.maxpool2 = MaxPool2d(kernel_size=3, stride=2)\\n\\n        self.fc1 = Linear(23 * 23 * 64, 40)\\n\\n        #self.conv1 = torch.nn.Conv2d(3, 64, kernel_size = 3, stride = 1, padding = 0)\\n        #self.conv2 = torch.nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 0)\\n        #self.maxpool = torch.nn.MaxPool2d(kernel_size = 3, stride = 1, padding = 0)\\n        #self.avgpool = torch.nn.AvgPool2d(kernel_size=66)\\n\\n        #self.bn1 = torch.nn.BatchNorm2d(64)\\n        #self.bn2 = torch.nn.BatchNorm2d(128)\\n        #self.bn3 = torch.nn.BatchNorm2d(256)\\n        #self.bn4 = torch.nn.BatchNorm2d(512)\\n\\n        #self.separableconv1 = SeparableConv2d(64,128)\\n        #self.separableconv2 = SeparableConv2d(128,128)\\n        #self.separableconv3 = SeparableConv2d(128,256)\\n        #self.separableconv4 = SeparableConv2d(256,256)\\n        #self.separableconv5 = SeparableConv2d(256,512)\\n\\n        #self.drop_layer = nn.Dropout(p=0.15)\\n        #self.fc1 = torch.nn.Linear(512, 40) # Should change 64 part\\n        self.softmax = nn.Softmax(dim=1)\\n\\n    # Defining the forward pass    \\n    def forward(self, x):\\n        x = self.conv1(x)\\n        x = self.maxpool1(x)\\n        x = self.conv2(x)\\n        x = self.maxpool2(x)\\n\\n        x = x.view(x.size(0), -1)\\n        x = self.fc1(x)\\n        # x = self.softmax(x) # Not a good idea (the model is trained really slow)\\n\\n        return x\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mik3vFrCSeTd"
      },
      "source": [
        "class Glyphnet(torch.nn.Module):\n",
        "   def __init__(self):\n",
        "      super(Glyphnet, self).__init__()\n",
        "      \n",
        "      self.conv1 = torch.nn.Conv2d(3, 64, kernel_size = 3, stride = 1, padding = 0)\n",
        "      self.conv2 = torch.nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 0)\n",
        "      self.maxpool = torch.nn.MaxPool2d(kernel_size = 3, stride = 1, padding = 0)\n",
        "      self.avgpool = torch.nn.AvgPool2d(kernel_size=66)\n",
        "\n",
        "      self.bn1 = torch.nn.BatchNorm2d(64)\n",
        "      self.bn2 = torch.nn.BatchNorm2d(128)\n",
        "      self.bn3 = torch.nn.BatchNorm2d(256)\n",
        "      self.bn4 = torch.nn.BatchNorm2d(512)\n",
        "\n",
        "      self.separableconv1 = SeparableConv2d(64,128)\n",
        "      self.separableconv2 = SeparableConv2d(128,128)\n",
        "      self.separableconv3 = SeparableConv2d(128,256)\n",
        "      self.separableconv4 = SeparableConv2d(256,256)\n",
        "      self.separableconv5 = SeparableConv2d(256,512)\n",
        "\n",
        "      self.drop_layer = nn.Dropout(p=0.15)\n",
        "      self.fc1 = torch.nn.Linear(66*66*512, 40) # Should change 64 part\n",
        "      self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "   def forward(self, x): # Input: 100 x 100 x 3\n",
        "      \n",
        "      # First block\n",
        "      x = self.bn1(self.conv1(x)) # Output: 98 x 98 x 64\n",
        "      x = F.relu(self.maxpool(x)) # Output: 96 x 96 x 64\n",
        "      x = self.bn1(self.conv2(x)) # Output: 94 x 94 x 64\n",
        "      x = F.relu(self.maxpool(x)) # Output: 92 x 92 x 64\n",
        "      \n",
        "      # Second block\n",
        "      x = F.relu(self.bn2(self.separableconv1(x))) # Output: 90 x 90 x 128\n",
        "      x = self.bn2(self.separableconv2(x)) # Output: 88 x 88 x 128\n",
        "      x = F.relu(self.maxpool(x)) # Output: 86 x 86 x 128\n",
        "\n",
        "      # Third block\n",
        "      x = F.relu(self.bn2(self.separableconv2(x))) # Output: 84 x 84 x 128\n",
        "      x = self.bn2(self.separableconv2(x)) # Output: 82 x 82 x 128\n",
        "      x = F.relu(self.maxpool(x)) # Output: 80 x 80 x 128\n",
        "\n",
        "      # Fourth block\n",
        "      x = F.relu(self.bn3(self.separableconv3(x))) # Output: 78 x 78 x 256\n",
        "      x = self.bn3(self.separableconv4(x)) # Output: 76 x 76 x 256\n",
        "      x = F.relu(self.maxpool(x)) # Output: 74 x 74 x 256\n",
        "\n",
        "      # Fifth block\n",
        "      x = F.relu(self.bn3(self.separableconv4(x))) # Output: 72 x 72 x 256\n",
        "      x = self.bn3(self.separableconv4(x)) # Output: 70 x 70 x 256\n",
        "      x = F.relu(self.maxpool(x)) # Output: 68 x 68 x 256\n",
        "\n",
        "      # Sixth block\n",
        "      x = F.relu(self.bn4(self.separableconv5(x))) # Output: 66 x 66 x 512\n",
        "\n",
        "      # x = self.avgpool(x) # Global Average Pooling; Output: 512\n",
        "\n",
        "      x = self.drop_layer(x) # Dropout Layer; Output: 512\n",
        "      \n",
        "      # Fully Connected Layer; Output: 512\n",
        "      x = x.view(x.size(0), -1)\n",
        "      x = self.fc1(x)\n",
        "\n",
        "      # x = self.softmax(x) # Softmax; Output: 512\n",
        "      \n",
        "      return(x)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkLXVJGkxemC"
      },
      "source": [
        "# Whether to extract features with the model\n",
        "feature_extract = False\n",
        "# Other selections\n",
        "loss_function = \"cross-entropy\"\n",
        "model_selection = \"glyphnet\"\n",
        "optim_selection = \"Adam\"\n",
        "\n",
        "# Load the model\n",
        "if model_selection == \"glyphnet\":\n",
        "    glyphnet = Glyphnet()\n",
        "\n",
        "# if GPU is available, move the model to GPU\n",
        "if train_on_gpu:\n",
        "    glyphnet.cuda()\n",
        "\n",
        "# Specify loss function (categorical cross-entropy)\n",
        "if loss_function == \"cross-entropy\":\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Specify optimizer (Adam) and learning rate = 0.001\n",
        "if optim_selection == \"Adam\":\n",
        "    optimizer = optim.Adam(glyphnet.parameters(), lr=0.001)\n",
        "\n",
        "# Exponential Decay to strengthen learning\n",
        "decayRate = 0.999\n",
        "my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2vnErB1yHmM"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOB03noWyEvQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6c2545e-9634-4971-b960-c1270e7e9548"
      },
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 50\n",
        "\n",
        "# The train_model function is from model_training python file\n",
        "glyphnet, train_losses = train_model(train_loader, optimizer, glyphnet, criterion, my_lr_scheduler, n_epochs)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 20 loss: 503.8103143930435408\n",
            "Epoch 1, Batch 40 loss: 106.8050039291381808\n",
            "Epoch 1, Batch 60 loss: 25.2648444175720215\n",
            "Epoch 1, Batch 80 loss: 7.2778836488723755\n",
            "Epoch 1, Batch 100 loss: 4.8998905777931210\n",
            "Epoch 1, Batch 120 loss: 3.0318630933761597\n",
            "Epoch 2, Batch 20 loss: 2.3345012009143828\n",
            "Epoch 2, Batch 40 loss: 2.3191961467266085\n",
            "Epoch 2, Batch 60 loss: 2.1083032131195067\n",
            "Epoch 2, Batch 80 loss: 4.9072824418544769\n",
            "Epoch 2, Batch 100 loss: 3.9134532630443575\n",
            "Epoch 2, Batch 120 loss: 4.3211573243141173\n",
            "Epoch 3, Batch 20 loss: 2.2051327347755434\n",
            "Epoch 3, Batch 40 loss: 2.0125870048999785\n",
            "Epoch 3, Batch 60 loss: 1.9946253120899200\n",
            "Epoch 3, Batch 80 loss: 2.1006899058818815\n",
            "Epoch 3, Batch 100 loss: 1.8797883152961732\n",
            "Epoch 3, Batch 120 loss: 1.9081024408340455\n",
            "Epoch 4, Batch 20 loss: 1.6850611627101899\n",
            "Epoch 4, Batch 40 loss: 1.8555836677551270\n",
            "Epoch 4, Batch 60 loss: 1.7946158051490784\n",
            "Epoch 4, Batch 80 loss: 1.6192289739847183\n",
            "Epoch 4, Batch 100 loss: 11.9566056519746784\n",
            "Epoch 4, Batch 120 loss: 14.2757535457611091\n",
            "Epoch 5, Batch 20 loss: 4.3282569229602812\n",
            "Epoch 5, Batch 40 loss: 3.0364151775836943\n",
            "Epoch 5, Batch 60 loss: 2.2053865909576418\n",
            "Epoch 5, Batch 80 loss: 2.0870480895042420\n",
            "Epoch 5, Batch 100 loss: 1.9449596703052521\n",
            "Epoch 5, Batch 120 loss: 1.7473812878131867\n",
            "Epoch 6, Batch 20 loss: 2.1124123394489289\n",
            "Epoch 6, Batch 40 loss: 1.7066168844699861\n",
            "Epoch 6, Batch 60 loss: 1.5349006593227386\n",
            "Epoch 6, Batch 80 loss: 1.5884679257869720\n",
            "Epoch 6, Batch 100 loss: 1.4691385060548783\n",
            "Epoch 6, Batch 120 loss: 1.5156638652086258\n",
            "Epoch 7, Batch 20 loss: 1.4032670438289643\n",
            "Epoch 7, Batch 40 loss: 1.4057997077703477\n",
            "Epoch 7, Batch 60 loss: 1.3063660830259323\n",
            "Epoch 7, Batch 80 loss: 1.5430596768856049\n",
            "Epoch 7, Batch 100 loss: 1.1686201632022857\n",
            "Epoch 7, Batch 120 loss: 1.3754570931196213\n",
            "Epoch 8, Batch 20 loss: 1.2711404412984848\n",
            "Epoch 8, Batch 40 loss: 1.4288454532623291\n",
            "Epoch 8, Batch 60 loss: 1.1231684595346452\n",
            "Epoch 8, Batch 80 loss: 1.1597522407770158\n",
            "Epoch 8, Batch 100 loss: 1.1720403164625168\n",
            "Epoch 8, Batch 120 loss: 1.1587790191173553\n",
            "Epoch 9, Batch 20 loss: 1.1727086335420609\n",
            "Epoch 9, Batch 40 loss: 1.2902627170085907\n",
            "Epoch 9, Batch 60 loss: 1.1694512486457824\n",
            "Epoch 9, Batch 80 loss: 0.8718132987618447\n",
            "Epoch 9, Batch 100 loss: 1.0450771421194076\n",
            "Epoch 9, Batch 120 loss: 1.0429345041513443\n",
            "Epoch 10, Batch 20 loss: 0.9969870582222938\n",
            "Epoch 10, Batch 40 loss: 1.0915314346551894\n",
            "Epoch 10, Batch 60 loss: 0.9718375891447067\n",
            "Epoch 10, Batch 80 loss: 1.1878626227378846\n",
            "Epoch 10, Batch 100 loss: 1.1509742647409440\n",
            "Epoch 10, Batch 120 loss: 1.0785071820020675\n",
            "Epoch 11, Batch 20 loss: 0.9584146291017532\n",
            "Epoch 11, Batch 40 loss: 1.0264210253953934\n",
            "Epoch 11, Batch 60 loss: 0.8896287873387336\n",
            "Epoch 11, Batch 80 loss: 0.9508516728878021\n",
            "Epoch 11, Batch 100 loss: 0.7785803288221359\n",
            "Epoch 11, Batch 120 loss: 0.8413853034377098\n",
            "Epoch 12, Batch 20 loss: 0.9461889743804932\n",
            "Epoch 12, Batch 40 loss: 0.8523067533969879\n",
            "Epoch 12, Batch 60 loss: 0.8598321050405502\n",
            "Epoch 12, Batch 80 loss: 0.8591718152165413\n",
            "Epoch 12, Batch 100 loss: 0.8875218778848648\n",
            "Epoch 12, Batch 120 loss: 0.7723712965846061\n",
            "Epoch 13, Batch 20 loss: 12.5532354980707161\n",
            "Epoch 13, Batch 40 loss: 1.6171015113592149\n",
            "Epoch 13, Batch 60 loss: 1.3285080403089524\n",
            "Epoch 13, Batch 80 loss: 1.1415862798690797\n",
            "Epoch 13, Batch 100 loss: 1.0915980309247970\n",
            "Epoch 13, Batch 120 loss: 1.1916424006223678\n",
            "Epoch 14, Batch 20 loss: 0.9686584472656250\n",
            "Epoch 14, Batch 40 loss: 0.9512072205543518\n",
            "Epoch 14, Batch 60 loss: 1.0173012703657149\n",
            "Epoch 14, Batch 80 loss: 0.8806387931108475\n",
            "Epoch 14, Batch 100 loss: 2.0178132325410845\n",
            "Epoch 14, Batch 120 loss: 1.2174962192773819\n",
            "Epoch 15, Batch 20 loss: 0.9757699966430664\n",
            "Epoch 15, Batch 40 loss: 0.9998057425022125\n",
            "Epoch 15, Batch 60 loss: 0.8482193663716316\n",
            "Epoch 15, Batch 80 loss: 0.6924960717558861\n",
            "Epoch 15, Batch 100 loss: 0.8943937152624131\n",
            "Epoch 15, Batch 120 loss: 0.9333559408783912\n",
            "Epoch 16, Batch 20 loss: 0.9435071572661400\n",
            "Epoch 16, Batch 40 loss: 0.9421855941414833\n",
            "Epoch 16, Batch 60 loss: 0.8323662966489792\n",
            "Epoch 16, Batch 80 loss: 0.7347973957657814\n",
            "Epoch 16, Batch 100 loss: 0.9292335152626038\n",
            "Epoch 16, Batch 120 loss: 0.8204968094825744\n",
            "Epoch 17, Batch 20 loss: 0.8228450611233711\n",
            "Epoch 17, Batch 40 loss: 0.8005695238709449\n",
            "Epoch 17, Batch 60 loss: 0.7108258634805680\n",
            "Epoch 17, Batch 80 loss: 0.8875189989805221\n",
            "Epoch 17, Batch 100 loss: 0.9024552300572395\n",
            "Epoch 17, Batch 120 loss: 0.7602231219410897\n",
            "Epoch 18, Batch 20 loss: 0.6248249769210815\n",
            "Epoch 18, Batch 40 loss: 0.7082264035940170\n",
            "Epoch 18, Batch 60 loss: 0.7007245987653732\n",
            "Epoch 18, Batch 80 loss: 0.7811579972505569\n",
            "Epoch 18, Batch 100 loss: 0.7580972343683243\n",
            "Epoch 18, Batch 120 loss: 0.7220283403992653\n",
            "Epoch 19, Batch 20 loss: 0.8365084826946259\n",
            "Epoch 19, Batch 40 loss: 0.7029604971408844\n",
            "Epoch 19, Batch 60 loss: 0.8133362337946892\n",
            "Epoch 19, Batch 80 loss: 0.8425098627805709\n",
            "Epoch 19, Batch 100 loss: 0.7590929910540580\n",
            "Epoch 19, Batch 120 loss: 0.6604089334607124\n",
            "Epoch 20, Batch 20 loss: 0.8217495918273926\n",
            "Epoch 20, Batch 40 loss: 0.7351003959774971\n",
            "Epoch 20, Batch 60 loss: 0.6747792482376098\n",
            "Epoch 20, Batch 80 loss: 0.6486024796962738\n",
            "Epoch 20, Batch 100 loss: 0.6193993330001831\n",
            "Epoch 20, Batch 120 loss: 0.8020754836499691\n",
            "Epoch 21, Batch 20 loss: 0.8112539604306221\n",
            "Epoch 21, Batch 40 loss: 0.7814135491847992\n",
            "Epoch 21, Batch 60 loss: 0.6997940137982368\n",
            "Epoch 21, Batch 80 loss: 0.7673507884144783\n",
            "Epoch 21, Batch 100 loss: 0.6136048890650272\n",
            "Epoch 21, Batch 120 loss: 0.6558477148413658\n",
            "Epoch 22, Batch 20 loss: 0.6567660570144653\n",
            "Epoch 22, Batch 40 loss: 0.7215172931551933\n",
            "Epoch 22, Batch 60 loss: 0.6313730746507644\n",
            "Epoch 22, Batch 80 loss: 0.6334225162863731\n",
            "Epoch 22, Batch 100 loss: 0.5203196264803409\n",
            "Epoch 22, Batch 120 loss: 0.8547135666012764\n",
            "Epoch 23, Batch 20 loss: 0.5602683290839195\n",
            "Epoch 23, Batch 40 loss: 0.6353255808353424\n",
            "Epoch 23, Batch 60 loss: 0.5484683312475681\n",
            "Epoch 23, Batch 80 loss: 0.6679536364972591\n",
            "Epoch 23, Batch 100 loss: 0.6744140587747097\n",
            "Epoch 23, Batch 120 loss: 0.6115759234875441\n",
            "Epoch 24, Batch 20 loss: 0.6762495845556259\n",
            "Epoch 24, Batch 40 loss: 0.6907022371888161\n",
            "Epoch 24, Batch 60 loss: 0.6579278096556663\n",
            "Epoch 24, Batch 80 loss: 0.5910299092531204\n",
            "Epoch 24, Batch 100 loss: 0.5496976315975189\n",
            "Epoch 24, Batch 120 loss: 0.5836400754749775\n",
            "Epoch 25, Batch 20 loss: 0.5377859503030777\n",
            "Epoch 25, Batch 40 loss: 0.5357329092919827\n",
            "Epoch 25, Batch 60 loss: 0.6712249696254731\n",
            "Epoch 25, Batch 80 loss: 0.6026447385549545\n",
            "Epoch 25, Batch 100 loss: 0.5925026036798954\n",
            "Epoch 25, Batch 120 loss: 0.6674744516611100\n",
            "Epoch 26, Batch 20 loss: 0.5880652397871018\n",
            "Epoch 26, Batch 40 loss: 0.5783567622303962\n",
            "Epoch 26, Batch 60 loss: 0.5399962060153485\n",
            "Epoch 26, Batch 80 loss: 0.5343615114688873\n",
            "Epoch 26, Batch 100 loss: 0.6804210335016251\n",
            "Epoch 26, Batch 120 loss: 0.5605835527181625\n",
            "Epoch 27, Batch 20 loss: 0.5977724507451058\n",
            "Epoch 27, Batch 40 loss: 0.7375731766223907\n",
            "Epoch 27, Batch 60 loss: 0.6267255350947381\n",
            "Epoch 27, Batch 80 loss: 0.5394504591822624\n",
            "Epoch 27, Batch 100 loss: 0.6134142875671387\n",
            "Epoch 27, Batch 120 loss: 0.5459695667028427\n",
            "Epoch 28, Batch 20 loss: 0.4821289703249931\n",
            "Epoch 28, Batch 40 loss: 0.5491341903805733\n",
            "Epoch 28, Batch 60 loss: 0.6404557317495346\n",
            "Epoch 28, Batch 80 loss: 0.5621837466955185\n",
            "Epoch 28, Batch 100 loss: 0.6138078525662423\n",
            "Epoch 28, Batch 120 loss: 0.5684490323066711\n",
            "Epoch 29, Batch 20 loss: 0.4647044263780117\n",
            "Epoch 29, Batch 40 loss: 0.6025851845741272\n",
            "Epoch 29, Batch 60 loss: 0.6127450853586197\n",
            "Epoch 29, Batch 80 loss: 0.5700195819139481\n",
            "Epoch 29, Batch 100 loss: 0.5735896512866020\n",
            "Epoch 29, Batch 120 loss: 0.6572531387209892\n",
            "Epoch 30, Batch 20 loss: 0.5602777570486068\n",
            "Epoch 30, Batch 40 loss: 0.4901248201727867\n",
            "Epoch 30, Batch 60 loss: 0.5155065815895796\n",
            "Epoch 30, Batch 80 loss: 0.5903110355138779\n",
            "Epoch 30, Batch 100 loss: 0.5984458580613137\n",
            "Epoch 30, Batch 120 loss: 0.5553342215716839\n",
            "Epoch 31, Batch 20 loss: 0.5926272355020046\n",
            "Epoch 31, Batch 40 loss: 0.6054002650082111\n",
            "Epoch 31, Batch 60 loss: 0.5375735521316528\n",
            "Epoch 31, Batch 80 loss: 0.5648429155349731\n",
            "Epoch 31, Batch 100 loss: 0.4148136153817177\n",
            "Epoch 31, Batch 120 loss: 0.5409158218652010\n",
            "Epoch 32, Batch 20 loss: 0.6188506111502647\n",
            "Epoch 32, Batch 40 loss: 0.4904561441391707\n",
            "Epoch 32, Batch 60 loss: 0.5989627175033092\n",
            "Epoch 32, Batch 80 loss: 0.5328677892684937\n",
            "Epoch 32, Batch 100 loss: 0.5584064483642578\n",
            "Epoch 32, Batch 120 loss: 0.5577811770141125\n",
            "Epoch 33, Batch 20 loss: 0.5154158048331737\n",
            "Epoch 33, Batch 40 loss: 0.5205805562436581\n",
            "Epoch 33, Batch 60 loss: 0.5744900546967984\n",
            "Epoch 33, Batch 80 loss: 0.6144042909145355\n",
            "Epoch 33, Batch 100 loss: 0.5791790686547756\n",
            "Epoch 33, Batch 120 loss: 0.5231524236500263\n",
            "Epoch 34, Batch 20 loss: 0.4345087245106697\n",
            "Epoch 34, Batch 40 loss: 0.5828488253057003\n",
            "Epoch 34, Batch 60 loss: 0.5579403266310692\n",
            "Epoch 34, Batch 80 loss: 0.4781978029757738\n",
            "Epoch 34, Batch 100 loss: 0.4994690068066120\n",
            "Epoch 34, Batch 120 loss: 0.5375943019986152\n",
            "Epoch 35, Batch 20 loss: 0.4535781741142273\n",
            "Epoch 35, Batch 40 loss: 0.6955446437001228\n",
            "Epoch 35, Batch 60 loss: 0.4245793066918850\n",
            "Epoch 35, Batch 80 loss: 0.6044196143746376\n",
            "Epoch 35, Batch 100 loss: 0.5763616934418678\n",
            "Epoch 35, Batch 120 loss: 0.4950099490582943\n",
            "Epoch 36, Batch 20 loss: 0.5631111405789853\n",
            "Epoch 36, Batch 40 loss: 0.4985934667289257\n",
            "Epoch 36, Batch 60 loss: 0.4899065077304840\n",
            "Epoch 36, Batch 80 loss: 0.5150103140622377\n",
            "Epoch 36, Batch 100 loss: 0.4425141833722591\n",
            "Epoch 36, Batch 120 loss: 0.5229317329823970\n",
            "Epoch 37, Batch 20 loss: 0.4216914474964142\n",
            "Epoch 37, Batch 40 loss: 0.6039094381034374\n",
            "Epoch 37, Batch 60 loss: 0.4317423045635224\n",
            "Epoch 37, Batch 80 loss: 0.6112908206880092\n",
            "Epoch 37, Batch 100 loss: 0.4302459135651588\n",
            "Epoch 37, Batch 120 loss: 0.5343244388699532\n",
            "Epoch 38, Batch 20 loss: 0.5526529498398304\n",
            "Epoch 38, Batch 40 loss: 0.5316926315426826\n",
            "Epoch 38, Batch 60 loss: 0.5553297668695449\n",
            "Epoch 38, Batch 80 loss: 0.5085363946855068\n",
            "Epoch 38, Batch 100 loss: 0.6083981752395630\n",
            "Epoch 38, Batch 120 loss: 0.5037502959370613\n",
            "Epoch 39, Batch 20 loss: 0.4824711751192808\n",
            "Epoch 39, Batch 40 loss: 0.4645200610160828\n",
            "Epoch 39, Batch 60 loss: 0.5781464874744415\n",
            "Epoch 39, Batch 80 loss: 0.5077432379126549\n",
            "Epoch 39, Batch 100 loss: 0.4785957202315331\n",
            "Epoch 39, Batch 120 loss: 0.4695510461926460\n",
            "Epoch 40, Batch 20 loss: 0.5718320310115814\n",
            "Epoch 40, Batch 40 loss: 0.4935915425419807\n",
            "Epoch 40, Batch 60 loss: 0.4677428208291531\n",
            "Epoch 40, Batch 80 loss: 0.4180142439901829\n",
            "Epoch 40, Batch 100 loss: 0.6273444406688213\n",
            "Epoch 40, Batch 120 loss: 0.4620819628238678\n",
            "Epoch 41, Batch 20 loss: 0.5006834004074335\n",
            "Epoch 41, Batch 40 loss: 0.5824546255171299\n",
            "Epoch 41, Batch 60 loss: 0.5273027904331684\n",
            "Epoch 41, Batch 80 loss: 0.4429265655577183\n",
            "Epoch 41, Batch 100 loss: 0.4431933358311653\n",
            "Epoch 41, Batch 120 loss: 0.4623401440680027\n",
            "Epoch 42, Batch 20 loss: 0.4147806011140346\n",
            "Epoch 42, Batch 40 loss: 0.4651676177978515\n",
            "Epoch 42, Batch 60 loss: 0.4790218137204647\n",
            "Epoch 42, Batch 80 loss: 0.5692128837108612\n",
            "Epoch 42, Batch 100 loss: 0.5728037636727095\n",
            "Epoch 42, Batch 120 loss: 0.5290651723742485\n",
            "Epoch 43, Batch 20 loss: 0.4514331400394440\n",
            "Epoch 43, Batch 40 loss: 0.5223347462713719\n",
            "Epoch 43, Batch 60 loss: 0.5396870881319046\n",
            "Epoch 43, Batch 80 loss: 0.4375258859246969\n",
            "Epoch 43, Batch 100 loss: 0.5141090057790280\n",
            "Epoch 43, Batch 120 loss: 0.4843122117221356\n",
            "Epoch 44, Batch 20 loss: 0.5578581482172013\n",
            "Epoch 44, Batch 40 loss: 0.4678306773304939\n",
            "Epoch 44, Batch 60 loss: 0.4285956963896752\n",
            "Epoch 44, Batch 80 loss: 0.5482503999024629\n",
            "Epoch 44, Batch 100 loss: 0.5239391840994359\n",
            "Epoch 44, Batch 120 loss: 0.6520211443305015\n",
            "Epoch 45, Batch 20 loss: 0.5598970539867878\n",
            "Epoch 45, Batch 40 loss: 0.5796794667840004\n",
            "Epoch 45, Batch 60 loss: 0.5701502837240696\n",
            "Epoch 45, Batch 80 loss: 0.4509761393070221\n",
            "Epoch 45, Batch 100 loss: 0.5047360740602016\n",
            "Epoch 45, Batch 120 loss: 0.4893468111753464\n",
            "Epoch 46, Batch 20 loss: 0.5574425786733628\n",
            "Epoch 46, Batch 40 loss: 0.5246118642389774\n",
            "Epoch 46, Batch 60 loss: 0.4705287255346775\n",
            "Epoch 46, Batch 80 loss: 0.5422684721648693\n",
            "Epoch 46, Batch 100 loss: 0.5055059600621462\n",
            "Epoch 46, Batch 120 loss: 0.4832564570009709\n",
            "Epoch 47, Batch 20 loss: 0.5265041187405586\n",
            "Epoch 47, Batch 40 loss: 0.4575201515108347\n",
            "Epoch 47, Batch 60 loss: 0.4413553684949875\n",
            "Epoch 47, Batch 80 loss: 0.5932188317179680\n",
            "Epoch 47, Batch 100 loss: 0.5200567767024040\n",
            "Epoch 47, Batch 120 loss: 0.5226550310850143\n",
            "Epoch 48, Batch 20 loss: 0.5399055093526840\n",
            "Epoch 48, Batch 40 loss: 0.4701161198318005\n",
            "Epoch 48, Batch 60 loss: 0.5082219377160072\n",
            "Epoch 48, Batch 80 loss: 0.4876385688781738\n",
            "Epoch 48, Batch 100 loss: 0.4653655901551247\n",
            "Epoch 48, Batch 120 loss: 0.6021242842078209\n",
            "Epoch 49, Batch 20 loss: 0.5778575450181961\n",
            "Epoch 49, Batch 40 loss: 0.4292042687535286\n",
            "Epoch 49, Batch 60 loss: 0.5275722473859787\n",
            "Epoch 49, Batch 80 loss: 0.5745775967836380\n",
            "Epoch 49, Batch 100 loss: 0.4822220794856548\n",
            "Epoch 49, Batch 120 loss: 0.4538252577185631\n",
            "Epoch 50, Batch 20 loss: 0.5464704930782318\n",
            "Epoch 50, Batch 40 loss: 0.4971724376082420\n",
            "Epoch 50, Batch 60 loss: 0.4661533497273922\n",
            "Epoch 50, Batch 80 loss: 0.4655744358897209\n",
            "Epoch 50, Batch 100 loss: 0.4820414036512375\n",
            "Epoch 50, Batch 120 loss: 0.3917975962162018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfA7ZjLR1BZP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "8072ca0d-860e-438a-e0f8-10efbe34df0e"
      },
      "source": [
        "plt.plot(train_losses)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItLyF9dQyJFL"
      },
      "source": [
        "Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGki98W3SoEW"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNavYOynyE6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1335dd9d-6954-4aab-f59a-b23b4c9a7434"
      },
      "source": [
        "# The test_model function is from model_testing python file\n",
        "test_loss, class_correct, class_total, labels, predictions = test_model(classes, glyphnet, test_loader, criterion)\n",
        "\n",
        "# Test accuracy for each hieroglyph\n",
        "for i in range(len(classes)):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (classes[i], 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "# Total Test accuracy\n",
        "print(\"\\nAccuracy: {:.3%}\".format(accuracy_score(labels, predictions)))\n",
        "print(\"\\nPrecision: {:.3%}\".format(precision_score(labels, predictions, average = 'weighted')))\n",
        "print(\"\\nRecall: {:.3%}\".format(recall_score(labels, predictions, average = 'weighted')))\n",
        "print(\"\\nF1-score: {:.3%}\".format(f1_score(labels, predictions, average = 'weighted')))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 8.654627\n",
            "\n",
            "Test Accuracy of    D2:  0% ( 0/ 5)\n",
            "Test Accuracy of   D21:  0% ( 0/36)\n",
            "Test Accuracy of   D36:  0% ( 0/12)\n",
            "Test Accuracy of    D4:  0% ( 0/ 8)\n",
            "Test Accuracy of   D46:  0% ( 0/11)\n",
            "Test Accuracy of   D58:  0% ( 0/ 8)\n",
            "Test Accuracy of   E23:  0% ( 0/ 2)\n",
            "Test Accuracy of   E34:  0% ( 0/25)\n",
            "Test Accuracy of   F31:  0% ( 0/ 2)\n",
            "Test Accuracy of   F35:  0% ( 0/ 1)\n",
            "Test Accuracy of    G1:  0% ( 0/ 7)\n",
            "Test Accuracy of   G17:  0% ( 0/39)\n",
            "Test Accuracy of   G43: 87% (35/40)\n",
            "Test Accuracy of   I10:  0% ( 0/ 9)\n",
            "Test Accuracy of    I9:  0% ( 0/30)\n",
            "Test Accuracy of   M17:  0% ( 0/73)\n",
            "Test Accuracy of   M23:  0% ( 0/ 8)\n",
            "Test Accuracy of   N35:  0% ( 0/90)\n",
            "Test Accuracy of    O1:  0% ( 0/ 4)\n",
            "Test Accuracy of   O34:  0% ( 0/ 4)\n",
            "Test Accuracy of    O4:  0% ( 0/ 2)\n",
            "Test Accuracy of   O49: 66% ( 2/ 3)\n",
            "Test Accuracy of    Q1:  0% ( 0/ 4)\n",
            "Test Accuracy of    Q3:  0% ( 0/16)\n",
            "Test Accuracy of    R4:  0% ( 0/ 1)\n",
            "Test Accuracy of    R8:  0% ( 0/14)\n",
            "Test Accuracy of   S29:  0% ( 0/53)\n",
            "Test Accuracy of   S34:  0% ( 0/ 2)\n",
            "Test Accuracy of    U7:  0% ( 0/ 1)\n",
            "Test Accuracy of   V13:  0% ( 0/16)\n",
            "Test Accuracy of   V28:  0% ( 0/ 8)\n",
            "Test Accuracy of   V30:  0% ( 0/ 2)\n",
            "Test Accuracy of   V31:  0% ( 0/27)\n",
            "Test Accuracy of   W11:  0% ( 0/ 1)\n",
            "Test Accuracy of   W24:  0% ( 0/ 8)\n",
            "Test Accuracy of    X1:  0% ( 0/47)\n",
            "Test Accuracy of    X8:  0% ( 0/ 1)\n",
            "Test Accuracy of    Y1:  0% ( 0/ 1)\n",
            "Test Accuracy of    Y5:  0% ( 0/ 2)\n",
            "Test Accuracy of    Z1:  0% ( 0/10)\n",
            "\n",
            "Accuracy: 5.845%\n",
            "\n",
            "Precision: 0.408%\n",
            "\n",
            "Recall: 5.845%\n",
            "\n",
            "F1-score: 0.763%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}