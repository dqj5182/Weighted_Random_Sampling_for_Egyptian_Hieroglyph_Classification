{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Egyptian_model_with_Ensemble_learning_3_ResNet_from_scratch_40_classes_WeightedRandomSampler.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8nXOuR-IIqk",
        "outputId": "b93e5dcf-a97e-4b67-afcd-b1d9f40732f8"
      },
      "source": [
        "!pip install torchensemble"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchensemble in /usr/local/lib/python3.7/dist-packages (0.1.6)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from torchensemble) (1.10.0+cu111)\n",
            "Requirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from torchensemble) (1.0.1)\n",
            "Requirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from torchensemble) (0.11.1+cu111)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.0->torchensemble) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.0->torchensemble) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.0->torchensemble) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.0->torchensemble) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->torchensemble) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.2.2->torchensemble) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnVsU472H-GM"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import os, os.path\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from pathlib import Path\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "from torchensemble import VotingClassifier\n",
        "from torchensemble.utils.logging import set_logger\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCkkbuJbLYEn",
        "outputId": "23afe238-c2d2-4b96-f26c-201a24b3fd30"
      },
      "source": [
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_L0mn9xLcJZ",
        "outputId": "7a36ad11-4767-4120-ced5-86905b642f9a"
      },
      "source": [
        "# Connecting and Mounting to the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ONZtfs_LSQQ"
      },
      "source": [
        "def get_class_distribution(dataset_obj, idx2class):\n",
        "    count_dict = {k:0 for k,v in dataset_obj.class_to_idx.items()}\n",
        "    \n",
        "    for element in dataset_obj:\n",
        "        y_lbl = element[1]\n",
        "        y_lbl = idx2class[y_lbl]\n",
        "        count_dict[y_lbl] += 1\n",
        "            \n",
        "    return count_dict"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwxUjnYtLS18"
      },
      "source": [
        "def load_data(hieroglyph_directory_path, batch_size=20, num_workers=0):\n",
        "    train_dir = os.path.join(hieroglyph_directory_path, 'train/')\n",
        "    test_dir = os.path.join(hieroglyph_directory_path, 'test/')\n",
        "\n",
        "    classes = []\n",
        "\n",
        "    for filename in os.listdir(train_dir):\n",
        "        if filename == '.DS_Store':\n",
        "            pass\n",
        "        else:\n",
        "            classes.append(filename)\n",
        "\n",
        "    classes.sort()\n",
        "\n",
        "    # print(\"Our classes:\", classes)\n",
        "    # print(len(classes))\n",
        "\n",
        "    data_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                            transforms.RandomApply([transforms.RandomHorizontalFlip()]),\n",
        "                                            transforms.RandomRotation(degrees=(-10, 10)),\n",
        "                                            transforms.RandomAffine(degrees=0, translate=(.1, .1)),\n",
        "                                            transforms.RandomApply([transforms.ColorJitter(brightness=(1, 1.2),\n",
        "                                                                                            contrast=(1, 1.5),\n",
        "                                                                                            saturation=(1, 1.5),\n",
        "                                                                                            hue=(0, 0.5))]),\n",
        "                                            transforms.RandomErasing(p=0.5, scale=(0.05, 0.05), ratio=(0.3, 3.3), value=0,\n",
        "                                                                      inplace=False),\n",
        "                                            transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "    train_data = datasets.ImageFolder(train_dir, transform=data_transform)\n",
        "    test_data = datasets.ImageFolder(test_dir, transform=data_transform)\n",
        "\n",
        "    # print('Num training images: ', len(train_data))\n",
        "    # print('Num test images: ', len(test_data))\n",
        "\n",
        "    # WeightedRandomSampler for train loader\n",
        "    idx2class = {v: k for k, v in train_data.class_to_idx.items()}\n",
        "\n",
        "    target_list = torch.tensor(train_data.targets)\n",
        "    class_count = [i for i in get_class_distribution(train_data, idx2class).values()]\n",
        "    class_weights = 1./torch.tensor(class_count, dtype=torch.float) \n",
        "    class_weights_all = class_weights[target_list]\n",
        "    weighted_sampler = torch.utils.data.WeightedRandomSampler(weights=class_weights_all, num_samples=len(class_weights_all), replacement=True)\n",
        "\n",
        "    # prepare data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "                                               num_workers=num_workers, sampler=weighted_sampler, drop_last=True)\n",
        "    \n",
        "    # WeightedRandomSampler for test loader\n",
        "    idx2class_test = {v: k for k, v in test_data.class_to_idx.items()}\n",
        "\n",
        "    target_list_test = torch.tensor(test_data.targets)\n",
        "    class_count_test = [i for i in get_class_distribution(test_data, idx2class_test).values()]\n",
        "    class_weights_test = 1./torch.tensor(class_count_test, dtype=torch.float) \n",
        "    class_weights_all_test = class_weights_test[target_list_test]\n",
        "    weighted_sampler_test = torch.utils.data.WeightedRandomSampler(weights=class_weights_all_test, num_samples=len(class_weights_all_test), replacement=True)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
        "                                              num_workers=num_workers, sampler=weighted_sampler_test)\n",
        "\n",
        "    return train_loader, test_loader, classes"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXC_WypPLdnL",
        "outputId": "2f2b6819-9d9a-4d0d-9d7a-8053e3be57e4"
      },
      "source": [
        "data_dir = '/content/drive/MyDrive/EgyptianHieroglyphDataset_Original_Clean/'\n",
        "\n",
        "hieroglyph_for_train = []\n",
        "file_count_list = []\n",
        "\n",
        "for name in os.listdir('/content/drive/MyDrive/EgyptianHieroglyphDataset_Original_Clean/train/'):\n",
        "  path, dirs, files = next(os.walk(\"/content/drive/MyDrive/EgyptianHieroglyphDataset_Original_Clean/train/\"+name))\n",
        "  file_count = len(files)\n",
        "  print(name, file_count)\n",
        "  file_count_list.append(file_count)\n",
        "  hieroglyph_for_train.append(name)\n",
        "\n",
        "hieroglyph_dict = dict(zip(hieroglyph_for_train, file_count_list))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y5 6\n",
            "I9 116\n",
            "O34 15\n",
            "V13 63\n",
            "U7 3\n",
            "D46 40\n",
            "E34 97\n",
            "G1 28\n",
            "V31 106\n",
            "S34 8\n",
            "D36 47\n",
            "Q1 13\n",
            "V30 6\n",
            "F35 2\n",
            "O4 11\n",
            "M23 30\n",
            "S29 212\n",
            "R8 53\n",
            "W11 4\n",
            "X1 185\n",
            "Y1 1\n",
            "D21 146\n",
            "D2 19\n",
            "E23 8\n",
            "X8 4\n",
            "Z1 39\n",
            "D4 29\n",
            "V28 28\n",
            "I10 32\n",
            "O1 16\n",
            "M17 291\n",
            "F31 6\n",
            "G43 157\n",
            "D58 28\n",
            "O49 10\n",
            "G17 156\n",
            "R4 2\n",
            "Q3 61\n",
            "W24 31\n",
            "N35 358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "sDWANMEdLhF9",
        "outputId": "999a1a3e-e4db-4ada-96c1-90d9d1d4b082"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\"Hieroglyph\":hieroglyph_for_train, \"Count\":file_count_list})\n",
        "\n",
        "df_sorted= df.sort_values('Count',ascending=False)\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "# make bar plot with matplotlib\n",
        "plt.bar('Hieroglyph', 'Count',data=df_sorted)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 40 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goisPc6eLi3L"
      },
      "source": [
        "# Number of images processed in a single training\n",
        "batch_size = 20\n",
        "num_workers = 0\n",
        "\n",
        "# The load_data function is from hieroglyph_data_preparation python file\n",
        "train_loader, test_loader, classes = load_data(data_dir)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIFr3iYkLrQ-"
      },
      "source": [
        "# Whether to extract features with the model\n",
        "feature_extract = False\n",
        "\n",
        "model_selection = \"resnet-50\"\n",
        "\n",
        "# False if you want scratch model, True if you want pretrained model\n",
        "whether_to_pretrain = False\n",
        "\n",
        "# Load the model\n",
        "if model_selection == \"resnet-50\":\n",
        "    resnet50 = models.resnet50(pretrained=whether_to_pretrain)\n",
        "\n",
        "# Number of features in the last layer of resnet\n",
        "n_inputs = resnet50.fc.in_features\n",
        "\n",
        "# Add last linear layer (n_inputs -> 40 hieroglyph classes)\n",
        "# New layers automatically have requires_grad = True\n",
        "last_layer = nn.Sequential(\n",
        "                nn.Linear(n_inputs, len(classes)))\n",
        "\n",
        "resnet50.fc = last_layer\n",
        "\n",
        "# if GPU is available, move the model to GPU\n",
        "if train_on_gpu:\n",
        "    resnet50.cuda()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Suq5RBD6K4O0",
        "outputId": "00764372-c817-4a5b-b206-ed6d1afd1a2d"
      },
      "source": [
        "epochs = 30\n",
        "\n",
        "# Define the ensemble\n",
        "ensemble = VotingClassifier(\n",
        "    estimator=resnet50,               # here is your deep learning model\n",
        "    n_estimators=3,                        # number of base estimators\n",
        "    cuda=True\n",
        ")\n",
        "\n",
        "# Set the optimizer\n",
        "ensemble.set_optimizer(\n",
        "    \"Adam\",                                 # type of parameter optimizer\n",
        "    lr=1e-3,                       # learning rate of parameter optimizer\n",
        "    weight_decay=5e-4,              # weight decay of parameter optimizer\n",
        ")\n",
        "\n",
        "# Set the learning rate scheduler\n",
        "ensemble.set_scheduler(\n",
        "    \"CosineAnnealingLR\",                    # type of learning rate scheduler\n",
        "    T_max=epochs,                           # additional arguments on the scheduler\n",
        ")\n",
        "\n",
        "# Train the ensemble\n",
        "ensemble.fit(\n",
        "    train_loader,\n",
        "    epochs=epochs,                          # number of training epochs\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimator: 000 | Epoch: 000 | Batch: 000 | Loss: 4.19425 | Correct: 0/20\n",
            "Estimator: 000 | Epoch: 000 | Batch: 100 | Loss: 3.72008 | Correct: 1/20\n",
            "Estimator: 001 | Epoch: 000 | Batch: 000 | Loss: 4.03164 | Correct: 1/20\n",
            "Estimator: 001 | Epoch: 000 | Batch: 100 | Loss: 3.32599 | Correct: 3/20\n",
            "Estimator: 002 | Epoch: 000 | Batch: 000 | Loss: 4.06092 | Correct: 1/20\n",
            "Estimator: 002 | Epoch: 000 | Batch: 100 | Loss: 4.11065 | Correct: 0/20\n",
            "Estimator: 000 | Epoch: 001 | Batch: 000 | Loss: 2.95241 | Correct: 3/20\n",
            "Estimator: 000 | Epoch: 001 | Batch: 100 | Loss: 1.84059 | Correct: 9/20\n",
            "Estimator: 001 | Epoch: 001 | Batch: 000 | Loss: 3.24114 | Correct: 0/20\n",
            "Estimator: 001 | Epoch: 001 | Batch: 100 | Loss: 2.97599 | Correct: 7/20\n",
            "Estimator: 002 | Epoch: 001 | Batch: 000 | Loss: 3.94600 | Correct: 1/20\n",
            "Estimator: 002 | Epoch: 001 | Batch: 100 | Loss: 2.97977 | Correct: 2/20\n",
            "Estimator: 000 | Epoch: 002 | Batch: 000 | Loss: 2.14221 | Correct: 8/20\n",
            "Estimator: 000 | Epoch: 002 | Batch: 100 | Loss: 1.56736 | Correct: 6/20\n",
            "Estimator: 001 | Epoch: 002 | Batch: 000 | Loss: 2.02092 | Correct: 8/20\n",
            "Estimator: 001 | Epoch: 002 | Batch: 100 | Loss: 2.05815 | Correct: 7/20\n",
            "Estimator: 002 | Epoch: 002 | Batch: 000 | Loss: 3.19530 | Correct: 2/20\n",
            "Estimator: 002 | Epoch: 002 | Batch: 100 | Loss: 2.51068 | Correct: 4/20\n",
            "Estimator: 000 | Epoch: 003 | Batch: 000 | Loss: 1.34423 | Correct: 11/20\n",
            "Estimator: 000 | Epoch: 003 | Batch: 100 | Loss: 1.86908 | Correct: 9/20\n",
            "Estimator: 001 | Epoch: 003 | Batch: 000 | Loss: 1.51268 | Correct: 11/20\n",
            "Estimator: 001 | Epoch: 003 | Batch: 100 | Loss: 1.05230 | Correct: 14/20\n",
            "Estimator: 002 | Epoch: 003 | Batch: 000 | Loss: 1.64692 | Correct: 8/20\n",
            "Estimator: 002 | Epoch: 003 | Batch: 100 | Loss: 2.11483 | Correct: 6/20\n",
            "Estimator: 000 | Epoch: 004 | Batch: 000 | Loss: 1.44127 | Correct: 9/20\n",
            "Estimator: 000 | Epoch: 004 | Batch: 100 | Loss: 0.63566 | Correct: 16/20\n",
            "Estimator: 001 | Epoch: 004 | Batch: 000 | Loss: 1.26854 | Correct: 10/20\n",
            "Estimator: 001 | Epoch: 004 | Batch: 100 | Loss: 1.08815 | Correct: 15/20\n",
            "Estimator: 002 | Epoch: 004 | Batch: 000 | Loss: 1.53201 | Correct: 10/20\n",
            "Estimator: 002 | Epoch: 004 | Batch: 100 | Loss: 1.01988 | Correct: 12/20\n",
            "Estimator: 000 | Epoch: 005 | Batch: 000 | Loss: 0.94707 | Correct: 12/20\n",
            "Estimator: 000 | Epoch: 005 | Batch: 100 | Loss: 0.51633 | Correct: 16/20\n",
            "Estimator: 001 | Epoch: 005 | Batch: 000 | Loss: 0.82921 | Correct: 15/20\n",
            "Estimator: 001 | Epoch: 005 | Batch: 100 | Loss: 0.95217 | Correct: 13/20\n",
            "Estimator: 002 | Epoch: 005 | Batch: 000 | Loss: 0.85770 | Correct: 12/20\n",
            "Estimator: 002 | Epoch: 005 | Batch: 100 | Loss: 0.93892 | Correct: 13/20\n",
            "Estimator: 000 | Epoch: 006 | Batch: 000 | Loss: 0.62558 | Correct: 17/20\n",
            "Estimator: 000 | Epoch: 006 | Batch: 100 | Loss: 0.54433 | Correct: 17/20\n",
            "Estimator: 001 | Epoch: 006 | Batch: 000 | Loss: 0.76226 | Correct: 15/20\n",
            "Estimator: 001 | Epoch: 006 | Batch: 100 | Loss: 0.92390 | Correct: 13/20\n",
            "Estimator: 002 | Epoch: 006 | Batch: 000 | Loss: 0.99896 | Correct: 13/20\n",
            "Estimator: 002 | Epoch: 006 | Batch: 100 | Loss: 0.78668 | Correct: 15/20\n",
            "Estimator: 000 | Epoch: 007 | Batch: 000 | Loss: 0.46209 | Correct: 16/20\n",
            "Estimator: 000 | Epoch: 007 | Batch: 100 | Loss: 0.60927 | Correct: 17/20\n",
            "Estimator: 001 | Epoch: 007 | Batch: 000 | Loss: 0.85628 | Correct: 15/20\n",
            "Estimator: 001 | Epoch: 007 | Batch: 100 | Loss: 0.38722 | Correct: 16/20\n",
            "Estimator: 002 | Epoch: 007 | Batch: 000 | Loss: 1.04946 | Correct: 13/20\n",
            "Estimator: 002 | Epoch: 007 | Batch: 100 | Loss: 0.90266 | Correct: 15/20\n",
            "Estimator: 000 | Epoch: 008 | Batch: 000 | Loss: 0.37275 | Correct: 17/20\n",
            "Estimator: 000 | Epoch: 008 | Batch: 100 | Loss: 0.34086 | Correct: 18/20\n",
            "Estimator: 001 | Epoch: 008 | Batch: 000 | Loss: 0.34101 | Correct: 20/20\n",
            "Estimator: 001 | Epoch: 008 | Batch: 100 | Loss: 0.46661 | Correct: 17/20\n",
            "Estimator: 002 | Epoch: 008 | Batch: 000 | Loss: 0.79640 | Correct: 13/20\n",
            "Estimator: 002 | Epoch: 008 | Batch: 100 | Loss: 1.07649 | Correct: 13/20\n",
            "Estimator: 000 | Epoch: 009 | Batch: 000 | Loss: 0.38137 | Correct: 16/20\n",
            "Estimator: 000 | Epoch: 009 | Batch: 100 | Loss: 0.44977 | Correct: 17/20\n",
            "Estimator: 001 | Epoch: 009 | Batch: 000 | Loss: 0.28104 | Correct: 19/20\n",
            "Estimator: 001 | Epoch: 009 | Batch: 100 | Loss: 0.82031 | Correct: 15/20\n",
            "Estimator: 002 | Epoch: 009 | Batch: 000 | Loss: 0.45380 | Correct: 17/20\n",
            "Estimator: 002 | Epoch: 009 | Batch: 100 | Loss: 0.66108 | Correct: 15/20\n",
            "Estimator: 000 | Epoch: 010 | Batch: 000 | Loss: 0.23589 | Correct: 19/20\n",
            "Estimator: 000 | Epoch: 010 | Batch: 100 | Loss: 0.33081 | Correct: 16/20\n",
            "Estimator: 001 | Epoch: 010 | Batch: 000 | Loss: 0.29409 | Correct: 18/20\n",
            "Estimator: 001 | Epoch: 010 | Batch: 100 | Loss: 0.31257 | Correct: 16/20\n",
            "Estimator: 002 | Epoch: 010 | Batch: 000 | Loss: 0.94189 | Correct: 13/20\n",
            "Estimator: 002 | Epoch: 010 | Batch: 100 | Loss: 0.58848 | Correct: 17/20\n",
            "Estimator: 000 | Epoch: 011 | Batch: 000 | Loss: 0.43775 | Correct: 18/20\n",
            "Estimator: 000 | Epoch: 011 | Batch: 100 | Loss: 0.60702 | Correct: 15/20\n",
            "Estimator: 001 | Epoch: 011 | Batch: 000 | Loss: 0.39777 | Correct: 17/20\n",
            "Estimator: 001 | Epoch: 011 | Batch: 100 | Loss: 0.41673 | Correct: 17/20\n",
            "Estimator: 002 | Epoch: 011 | Batch: 000 | Loss: 0.41321 | Correct: 16/20\n",
            "Estimator: 002 | Epoch: 011 | Batch: 100 | Loss: 0.55314 | Correct: 17/20\n",
            "Estimator: 000 | Epoch: 012 | Batch: 000 | Loss: 0.62248 | Correct: 15/20\n",
            "Estimator: 000 | Epoch: 012 | Batch: 100 | Loss: 0.65886 | Correct: 15/20\n",
            "Estimator: 001 | Epoch: 012 | Batch: 000 | Loss: 0.19701 | Correct: 18/20\n",
            "Estimator: 001 | Epoch: 012 | Batch: 100 | Loss: 0.39416 | Correct: 17/20\n",
            "Estimator: 002 | Epoch: 012 | Batch: 000 | Loss: 0.21212 | Correct: 20/20\n",
            "Estimator: 002 | Epoch: 012 | Batch: 100 | Loss: 0.67132 | Correct: 14/20\n",
            "Estimator: 000 | Epoch: 013 | Batch: 000 | Loss: 0.44518 | Correct: 16/20\n",
            "Estimator: 000 | Epoch: 013 | Batch: 100 | Loss: 0.27887 | Correct: 18/20\n",
            "Estimator: 001 | Epoch: 013 | Batch: 000 | Loss: 0.51742 | Correct: 16/20\n",
            "Estimator: 001 | Epoch: 013 | Batch: 100 | Loss: 0.29084 | Correct: 18/20\n",
            "Estimator: 002 | Epoch: 013 | Batch: 000 | Loss: 0.30344 | Correct: 18/20\n",
            "Estimator: 002 | Epoch: 013 | Batch: 100 | Loss: 0.23690 | Correct: 18/20\n",
            "Estimator: 000 | Epoch: 014 | Batch: 000 | Loss: 0.57487 | Correct: 15/20\n",
            "Estimator: 000 | Epoch: 014 | Batch: 100 | Loss: 0.46354 | Correct: 17/20\n",
            "Estimator: 001 | Epoch: 014 | Batch: 000 | Loss: 0.52885 | Correct: 18/20\n",
            "Estimator: 001 | Epoch: 014 | Batch: 100 | Loss: 0.25555 | Correct: 18/20\n",
            "Estimator: 002 | Epoch: 014 | Batch: 000 | Loss: 0.28025 | Correct: 18/20\n",
            "Estimator: 002 | Epoch: 014 | Batch: 100 | Loss: 0.41049 | Correct: 17/20\n",
            "Estimator: 000 | Epoch: 015 | Batch: 000 | Loss: 0.24189 | Correct: 18/20\n",
            "Estimator: 000 | Epoch: 015 | Batch: 100 | Loss: 0.37699 | Correct: 17/20\n",
            "Estimator: 001 | Epoch: 015 | Batch: 000 | Loss: 0.17458 | Correct: 19/20\n",
            "Estimator: 001 | Epoch: 015 | Batch: 100 | Loss: 0.59473 | Correct: 18/20\n",
            "Estimator: 002 | Epoch: 015 | Batch: 000 | Loss: 0.17041 | Correct: 19/20\n",
            "Estimator: 002 | Epoch: 015 | Batch: 100 | Loss: 0.31710 | Correct: 18/20\n",
            "Estimator: 000 | Epoch: 016 | Batch: 000 | Loss: 0.10733 | Correct: 19/20\n",
            "Estimator: 000 | Epoch: 016 | Batch: 100 | Loss: 0.33688 | Correct: 18/20\n",
            "Estimator: 001 | Epoch: 016 | Batch: 000 | Loss: 0.40604 | Correct: 15/20\n",
            "Estimator: 001 | Epoch: 016 | Batch: 100 | Loss: 0.12214 | Correct: 20/20\n",
            "Estimator: 002 | Epoch: 016 | Batch: 000 | Loss: 0.28309 | Correct: 19/20\n",
            "Estimator: 002 | Epoch: 016 | Batch: 100 | Loss: 0.49089 | Correct: 17/20\n",
            "Estimator: 000 | Epoch: 017 | Batch: 000 | Loss: 0.15293 | Correct: 18/20\n",
            "Estimator: 000 | Epoch: 017 | Batch: 100 | Loss: 0.04269 | Correct: 20/20\n",
            "Estimator: 001 | Epoch: 017 | Batch: 000 | Loss: 0.23092 | Correct: 19/20\n",
            "Estimator: 001 | Epoch: 017 | Batch: 100 | Loss: 0.24017 | Correct: 19/20\n",
            "Estimator: 002 | Epoch: 017 | Batch: 000 | Loss: 0.12767 | Correct: 20/20\n",
            "Estimator: 002 | Epoch: 017 | Batch: 100 | Loss: 0.09902 | Correct: 20/20\n",
            "Estimator: 000 | Epoch: 018 | Batch: 000 | Loss: 0.11095 | Correct: 20/20\n",
            "Estimator: 000 | Epoch: 018 | Batch: 100 | Loss: 0.43190 | Correct: 16/20\n",
            "Estimator: 001 | Epoch: 018 | Batch: 000 | Loss: 0.24941 | Correct: 18/20\n",
            "Estimator: 001 | Epoch: 018 | Batch: 100 | Loss: 0.26089 | Correct: 18/20\n",
            "Estimator: 002 | Epoch: 018 | Batch: 000 | Loss: 0.25012 | Correct: 18/20\n",
            "Estimator: 002 | Epoch: 018 | Batch: 100 | Loss: 0.19402 | Correct: 19/20\n",
            "Estimator: 000 | Epoch: 019 | Batch: 000 | Loss: 0.14503 | Correct: 19/20\n",
            "Estimator: 000 | Epoch: 019 | Batch: 100 | Loss: 0.17174 | Correct: 19/20\n",
            "Estimator: 001 | Epoch: 019 | Batch: 000 | Loss: 0.40898 | Correct: 18/20\n",
            "Estimator: 001 | Epoch: 019 | Batch: 100 | Loss: 0.31779 | Correct: 18/20\n",
            "Estimator: 002 | Epoch: 019 | Batch: 000 | Loss: 0.15044 | Correct: 19/20\n",
            "Estimator: 002 | Epoch: 019 | Batch: 100 | Loss: 0.15827 | Correct: 19/20\n",
            "Estimator: 000 | Epoch: 020 | Batch: 000 | Loss: 0.10435 | Correct: 20/20\n",
            "Estimator: 000 | Epoch: 020 | Batch: 100 | Loss: 0.16580 | Correct: 19/20\n",
            "Estimator: 001 | Epoch: 020 | Batch: 000 | Loss: 0.16066 | Correct: 19/20\n",
            "Estimator: 001 | Epoch: 020 | Batch: 100 | Loss: 0.11569 | Correct: 19/20\n",
            "Estimator: 002 | Epoch: 020 | Batch: 000 | Loss: 0.22745 | Correct: 19/20\n",
            "Estimator: 002 | Epoch: 020 | Batch: 100 | Loss: 0.20731 | Correct: 18/20\n",
            "Estimator: 000 | Epoch: 021 | Batch: 000 | Loss: 0.32160 | Correct: 17/20\n",
            "Estimator: 000 | Epoch: 021 | Batch: 100 | Loss: 0.13467 | Correct: 19/20\n",
            "Estimator: 001 | Epoch: 021 | Batch: 000 | Loss: 0.05641 | Correct: 20/20\n",
            "Estimator: 001 | Epoch: 021 | Batch: 100 | Loss: 0.10601 | Correct: 19/20\n",
            "Estimator: 002 | Epoch: 021 | Batch: 000 | Loss: 0.07577 | Correct: 20/20\n",
            "Estimator: 002 | Epoch: 021 | Batch: 100 | Loss: 0.37812 | Correct: 18/20\n",
            "Estimator: 000 | Epoch: 022 | Batch: 000 | Loss: 0.08968 | Correct: 20/20\n",
            "Estimator: 000 | Epoch: 022 | Batch: 100 | Loss: 0.08354 | Correct: 20/20\n",
            "Estimator: 001 | Epoch: 022 | Batch: 000 | Loss: 0.10945 | Correct: 19/20\n",
            "Estimator: 001 | Epoch: 022 | Batch: 100 | Loss: 0.24725 | Correct: 18/20\n",
            "Estimator: 002 | Epoch: 022 | Batch: 000 | Loss: 0.03912 | Correct: 20/20\n",
            "Estimator: 002 | Epoch: 022 | Batch: 100 | Loss: 0.10010 | Correct: 19/20\n",
            "Estimator: 000 | Epoch: 023 | Batch: 000 | Loss: 0.15138 | Correct: 19/20\n",
            "Estimator: 000 | Epoch: 023 | Batch: 100 | Loss: 0.25712 | Correct: 19/20\n",
            "Estimator: 001 | Epoch: 023 | Batch: 000 | Loss: 0.05211 | Correct: 20/20\n",
            "Estimator: 001 | Epoch: 023 | Batch: 100 | Loss: 0.10812 | Correct: 19/20\n",
            "Estimator: 002 | Epoch: 023 | Batch: 000 | Loss: 0.19129 | Correct: 18/20\n",
            "Estimator: 002 | Epoch: 023 | Batch: 100 | Loss: 0.07561 | Correct: 20/20\n",
            "Estimator: 000 | Epoch: 024 | Batch: 000 | Loss: 0.04549 | Correct: 20/20\n",
            "Estimator: 000 | Epoch: 024 | Batch: 100 | Loss: 0.02927 | Correct: 20/20\n",
            "Estimator: 001 | Epoch: 024 | Batch: 000 | Loss: 0.04948 | Correct: 19/20\n",
            "Estimator: 001 | Epoch: 024 | Batch: 100 | Loss: 0.19037 | Correct: 18/20\n",
            "Estimator: 002 | Epoch: 024 | Batch: 000 | Loss: 0.11422 | Correct: 19/20\n",
            "Estimator: 002 | Epoch: 024 | Batch: 100 | Loss: 0.04885 | Correct: 20/20\n",
            "Estimator: 000 | Epoch: 025 | Batch: 000 | Loss: 0.03298 | Correct: 20/20\n",
            "Estimator: 000 | Epoch: 025 | Batch: 100 | Loss: 0.00822 | Correct: 20/20\n",
            "Estimator: 001 | Epoch: 025 | Batch: 000 | Loss: 0.19898 | Correct: 19/20\n",
            "Estimator: 001 | Epoch: 025 | Batch: 100 | Loss: 0.03108 | Correct: 20/20\n",
            "Estimator: 002 | Epoch: 025 | Batch: 000 | Loss: 0.05766 | Correct: 20/20\n",
            "Estimator: 002 | Epoch: 025 | Batch: 100 | Loss: 0.05521 | Correct: 20/20\n",
            "Estimator: 000 | Epoch: 026 | Batch: 000 | Loss: 0.17385 | Correct: 19/20\n",
            "Estimator: 000 | Epoch: 026 | Batch: 100 | Loss: 0.14660 | Correct: 19/20\n",
            "Estimator: 001 | Epoch: 026 | Batch: 000 | Loss: 0.08873 | Correct: 19/20\n",
            "Estimator: 001 | Epoch: 026 | Batch: 100 | Loss: 0.02040 | Correct: 20/20\n",
            "Estimator: 002 | Epoch: 026 | Batch: 000 | Loss: 0.04220 | Correct: 20/20\n",
            "Estimator: 002 | Epoch: 026 | Batch: 100 | Loss: 0.22631 | Correct: 19/20\n",
            "Estimator: 000 | Epoch: 027 | Batch: 000 | Loss: 0.02132 | Correct: 20/20\n",
            "Estimator: 000 | Epoch: 027 | Batch: 100 | Loss: 0.17853 | Correct: 19/20\n",
            "Estimator: 001 | Epoch: 027 | Batch: 000 | Loss: 0.06981 | Correct: 20/20\n",
            "Estimator: 001 | Epoch: 027 | Batch: 100 | Loss: 0.00499 | Correct: 20/20\n",
            "Estimator: 002 | Epoch: 027 | Batch: 000 | Loss: 0.18720 | Correct: 19/20\n",
            "Estimator: 002 | Epoch: 027 | Batch: 100 | Loss: 0.19267 | Correct: 18/20\n",
            "Estimator: 000 | Epoch: 028 | Batch: 000 | Loss: 0.07181 | Correct: 19/20\n",
            "Estimator: 000 | Epoch: 028 | Batch: 100 | Loss: 0.05084 | Correct: 20/20\n",
            "Estimator: 001 | Epoch: 028 | Batch: 000 | Loss: 0.04910 | Correct: 19/20\n",
            "Estimator: 001 | Epoch: 028 | Batch: 100 | Loss: 0.00705 | Correct: 20/20\n",
            "Estimator: 002 | Epoch: 028 | Batch: 000 | Loss: 0.20855 | Correct: 19/20\n",
            "Estimator: 002 | Epoch: 028 | Batch: 100 | Loss: 0.09033 | Correct: 19/20\n",
            "Estimator: 000 | Epoch: 029 | Batch: 000 | Loss: 0.04890 | Correct: 20/20\n",
            "Estimator: 000 | Epoch: 029 | Batch: 100 | Loss: 0.02486 | Correct: 20/20\n",
            "Estimator: 001 | Epoch: 029 | Batch: 000 | Loss: 0.00899 | Correct: 20/20\n",
            "Estimator: 001 | Epoch: 029 | Batch: 100 | Loss: 0.01422 | Correct: 20/20\n",
            "Estimator: 002 | Epoch: 029 | Batch: 000 | Loss: 0.04101 | Correct: 20/20\n",
            "Estimator: 002 | Epoch: 029 | Batch: 100 | Loss: 0.02658 | Correct: 20/20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oeFAcQiTSlU"
      },
      "source": [
        "def test_model(classes, resnet50, test_loader, criterion):\n",
        "    # track test loss\n",
        "    test_loss = 0.0\n",
        "    class_correct = list(0. for i in range(len(classes)))\n",
        "    class_total = list(0. for i in range(len(classes)))\n",
        "\n",
        "    resnet50.eval()  # eval mode\n",
        "\n",
        "    labels = []\n",
        "    predictions = []\n",
        "    # iterate over test data\n",
        "    for data, target in test_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = resnet50(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # update  test loss\n",
        "        test_loss += loss.item() * data.size(0)\n",
        "        # convert output probabilities to predicted class\n",
        "        _, pred = torch.max(output, 1)\n",
        "        # compare predictions to true label\n",
        "        correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "        # calculate test accuracy for each object class\n",
        "        for i in range(len(target.data)):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "        \n",
        "        # Will be used for calculating Recall, Precision, and F1-score\n",
        "        labels.extend(target.data.view_as(pred).tolist())\n",
        "        predictions.extend(pred.tolist())\n",
        "\n",
        "\n",
        "    # calculate avg test loss\n",
        "    test_loss = test_loss / len(test_loader.dataset)\n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "    return test_loss, class_correct, class_total, labels, predictions"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYJie1OQTj7x"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-cBRomUTUSW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f862ca3-6ba6-4077-e060-e5d4142d0a84"
      },
      "source": [
        "test_loss, class_correct, class_total, labels, predictions = test_model(classes, ensemble, test_loader, criterion)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 2.803503\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae_JIsO3Ts2f"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PudutYWPTp5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa32835a-da15-40a9-82ab-d70d24f59536"
      },
      "source": [
        "# Test accuracy for each hieroglyph\n",
        "for i in range(len(classes)):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (classes[i], 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "# Total Test accuracy\n",
        "print(\"\\nAccuracy: {:.3%}\".format(accuracy_score(labels, predictions)))\n",
        "print(\"\\nPrecision: {:.3%}\".format(precision_score(labels, predictions, average = 'weighted')))\n",
        "print(\"\\nRecall: {:.3%}\".format(recall_score(labels, predictions, average = 'weighted')))\n",
        "print(\"\\nF1-score: {:.3%}\".format(f1_score(labels, predictions, average = 'weighted')))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of    D2: 100% (20/20)\n",
            "Test Accuracy of   D21: 94% (16/17)\n",
            "Test Accuracy of   D36: 100% (14/14)\n",
            "Test Accuracy of    D4: 93% (15/16)\n",
            "Test Accuracy of   D46: 90% (18/20)\n",
            "Test Accuracy of   D58: 100% (18/18)\n",
            "Test Accuracy of   E23: 86% (13/15)\n",
            "Test Accuracy of   E34: 100% (17/17)\n",
            "Test Accuracy of   F31: 100% (19/19)\n",
            "Test Accuracy of   F35: 100% (25/25)\n",
            "Test Accuracy of    G1: 93% (14/15)\n",
            "Test Accuracy of   G17: 93% (15/16)\n",
            "Test Accuracy of   G43: 100% ( 9/ 9)\n",
            "Test Accuracy of   I10: 100% (15/15)\n",
            "Test Accuracy of    I9: 100% (14/14)\n",
            "Test Accuracy of   M17: 88% (15/17)\n",
            "Test Accuracy of   M23: 100% (13/13)\n",
            "Test Accuracy of   N35: 100% (18/18)\n",
            "Test Accuracy of    O1: 100% (13/13)\n",
            "Test Accuracy of   O34: 100% (18/18)\n",
            "Test Accuracy of    O4: 93% (14/15)\n",
            "Test Accuracy of   O49: 100% (13/13)\n",
            "Test Accuracy of    Q1: 85% (12/14)\n",
            "Test Accuracy of    Q3: 100% (11/11)\n",
            "Test Accuracy of    R4: 100% (18/18)\n",
            "Test Accuracy of    R8: 94% (16/17)\n",
            "Test Accuracy of   S29: 100% (19/19)\n",
            "Test Accuracy of   S34: 100% (11/11)\n",
            "Test Accuracy of    U7: 100% (15/15)\n",
            "Test Accuracy of   V13: 100% (17/17)\n",
            "Test Accuracy of   V28: 91% (11/12)\n",
            "Test Accuracy of   V30: 94% (16/17)\n",
            "Test Accuracy of   V31: 84% (16/19)\n",
            "Test Accuracy of   W11: 100% (19/19)\n",
            "Test Accuracy of   W24: 100% (13/13)\n",
            "Test Accuracy of    X1: 82% (14/17)\n",
            "Test Accuracy of    X8: 100% (14/14)\n",
            "Test Accuracy of    Y1: 100% (10/10)\n",
            "Test Accuracy of    Y5: 100% (15/15)\n",
            "Test Accuracy of    Z1: 100% (19/19)\n",
            "\n",
            "Accuracy: 96.530%\n",
            "\n",
            "Precision: 96.748%\n",
            "\n",
            "Recall: 96.530%\n",
            "\n",
            "F1-score: 96.526%\n"
          ]
        }
      ]
    }
  ]
}